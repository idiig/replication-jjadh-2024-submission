---
title: 古今和歌集の現代日本語訳の分析 - 追加率と拡張意味単位の観点から
author:
  - name: Xudong Chen
    corresponding: false
    roles: []
    id: jc
    orcid: 0000-0002-4542-2878
    email: xchen@shs.ens.titech.ac.jp
    affiliation:
      - name: Tokyo Institute of Technology
        city: Tokyo
        country: Japan
        url: 'https://www.titech.ac.jp/english/'
        isni: 0000000121792105
        ror: 0112mx960
  - name: Bor Hodošček
    corresponding: false
    roles: []
    id: jc
    orcid: 0000-0003-2246-8774
    email: hodoscek.bor.hmt@osaka-u.ac.jp
    affiliation:
      - name: Osaka University
        city: Osaka
        country: Japan
        url: 'https://www.osaka-u.ac.jp/en'
        isni: 0000000403733971
        ror: 035t8zc32
  - name: Hilofumi Yamamoto
    corresponding: true
    roles: []
    id: jc
    orcid: 0000-0001-6876-139X
    email: yamagen@lia.titech.ac.jp
    affiliation:
      - name: Tokyo Institute of Technology
        city: Tokyo
        country: Japan
        url: 'https://www.titech.ac.jp/english/'
        isni: 0000000121792105
        ror: 0112mx960
date: 2024/09/01
abstract: |
  本論文の目的は、原文との一致率の計算と意味拡張単位の翻訳という二つの側面から、和歌の現代語訳が、たとえ直訳・逐語訳に拘っているとしても、和歌辞典とは異なる解釈材料として、和歌の非字義的な情報を補足説明するために使用できることを示すことである。本研究の貢献は以下にある（仮）。1. 古今和歌集の現代語訳における初の語彙コーパスを公開した。2. 古今和歌集の翻訳の歴史的変遷を俯瞰し、特に20世紀における和歌の現代語訳の翻訳方針および翻訳観の整理を行った。3. 古語と現代語の翻訳対応率の計算により、翻訳方針にかかわらず、現代語訳において情報が追加されていることを示した。4. 意味拡張単位の観点に基づいた事例研究では、collocation（語と語の共出現）のノンリテラルなレベルでは現代語訳では追加はせず、入れ替えも比較的一貫している。対して、5. coligation（語とグラマレィカルパターンとの共出現）、semantic preference（語と特定意味領域との共出現）といったノンリテラルなレベルでは、原文に対してより多様な入れ替えと追加が行われ、対応づけが多様化していくことを明らかにした。以上により、現代語訳をノンリテラルレベルの可視化に応用すると考えられる。
plain-language-summary: |
  Test
key-points:
  - A web scraping script was developed to pull data from the Instituto Geogràphico Nacional into a machine-readable form for analysis
  - Earthquake events on La Palma are consistent with the presence of both mantle and crustal reservoirs.
citation:
  container-title: Journal of Japanese Association for Digital Humanities
  volume: 0
  issue: 0
  doi: 10.17928/jjadh.0.0_0
keywords:
  - Classic Japanese Poetry
  - Translation
  - Alignment
  - Extended Unit of Meaning
license: CC BY
copyright:
  holder: 'Xudong Chen, Bor Hodošček, Hilofumi Yamamoto'
  year: 2024
funding: 'This work was supported by JSPS KAKENHI Grant Number JP18K00528 and JP23KJ0910, JP23K00545.'
format:
  html:
    code-links:
      - text: Data Import Code
        icon: file-code
        href: 'https://github.com/idiig/jjadh-replication/tree/main'
    theme: default
    toc: true
    toc-title: 目次
    toc-location: right-body
    number-sections: true
    html-math-method: katex
    fig_caption: true
    cap-location: margin
    reference-location: margin
    citation-location: document
    code-fold: true
    fig-path: artifacts/figures
    fig-format: svg
    fig-width: 6
    fig-height: 3.71
  pdf:
    toc: true
    toc-title: 目次
    number-sections: true
    fig_caption: true
    fig-path: artifacts/figures
    fig-format: svg
    fig-width: 6
    fig-height: 3.71
    pdf-engine: xelatex
    citation-location: document
  docx:
    toc: true
    toc-title: 目次
    number-sections: true
    fig_caption: true
    fig-path: artifacts/figures
    fig-format: png
    fig-width: 6
    fig-height: 3.71
    citation-location: document
execute:
  freeze: auto
  message: false
bibliography: references.bib
crossref:
  fig-title: Figure
  tbl-title: Table
  title-delim: ':\quad'
  fig-prefix: Figure
  tbl-prefix: Table
  sec-prefix: Section
  eq-prefix: Eq.
---

:::{.callout-important title="Note"}
test


```{R}
#| label: libraries
#| message: false
library(fitdistrplus)
library(tidyverse)
library(knitr)
library(rstatix)

library(brms)
library(tidybayes)
library(scales)
library(ggdist)
library(ggplot2)
library(ggpubr)
library(ggridges)
library(ggokabeito)
```

```{R}
#| label: themes
theme_set_b <- function() {
  theme_void() +
    theme(
      strip.background = element_rect(
        color = "white", 
        fill = "white"
      ),
      panel.grid.major.y = element_line(
        color = "gray80",
        linetype = "solid"
      ),
      strip.text = element_text(
        color = "black",
        size = rel(1),
        angle = 90,
        vjust = 0.5,
        hjust = 1
      ),
      axis.text.y = element_text(
        color = "black", 
        hjust = 0
      ),
      strip.text.y.left = element_text(
        angle = 180
      ),
      axis.title.x = element_text(
        size = rel(1.3)
      ),
      axis.title.y = element_text(
        size = rel(1.3),
        angle = 90,
        vjust = 0.5
      ),
      legend.position = "bottom",
      legend.title = element_text(
        size = rel(1),
        face = "bold"
      )
    )
}
```
:::

# はじめに

## 目的と結論の概要

本論文の目的は、原文との一致率の計算と意味拡張単位の翻訳という二つの側面から、和歌の現代語訳が、たとえ直訳・逐語訳に拘っているとしても、和歌辞典とは異なる解釈材料として、和歌の非字義的な情報を補足説明するために使用できることを示すことである。

本研究の貢献は以下にある（仮）。

1. 古今和歌集の現代語訳における初の語彙コーパスを公開した。
2. 古今和歌集の翻訳の歴史的変遷を俯瞰し、特に20世紀における和歌の現代語訳の翻訳方針および翻訳観の整理を行った。
3. 古語と現代語の翻訳対応率の計算により、翻訳方針にかかわらず、現代語訳において情報が追加されていることを示した。
4. 意味拡張単位の観点に基づいた事例研究では、collocation（語と語の共出現）のノンリテラルなレベルでは現代語訳では追加はせず、入れ替えも比較的一貫している。対して、
5. coligation（語とグラマレィカルパターンとの共出現）、semantic preference（語と特定意味領域との共出現）といったノンリテラルなレベルでは、原文に対してより多様な入れ替えと追加が行われ、対応づけが多様化していくことを明らかにした。

以上により、現代語訳をノンリテラルレベルの可視化に応用すると考えられる。

## 背景

### 和歌と和歌における非明示的要素

古今和歌集仮名序にあるように、歌人が「心に思ふもの」を自然界の物事に託しており、和歌を字義通り理解したところでその「心に思ふもの」には直接アクセスしているとはいえない。古今集の恋の歌を例にあげよう：

> 初雁の鳴きこそわたれ世の中の人の心の秋しうければ (古今・恋五・貫之)

このテキストを読むとして、それぞれの語の字義的・リテラルな意味がわかれば、文字通り^[初雁が鳴いて渡ってくるのだが、人の心の秋が来るのが悲しいので（筆者訳）。]には理解できる。しかし、歌ことば辞典で「秋【あき】」[@katagiri1983Uta, 3] を調べると、リテラルな意味だけでは読まれていない情報があることに気付く。

> 秋【あき】 […]「秋」と「飽」を掛け、過ぎ去ってゆく秋と過ぎ去ってゆく愛を惜しむことが多かった。【脚注】[雁の]「鳴く」を人が「泣く」と同列にしか把握しない […]

この記述に従い読み解いていけば、恋人の心が「飽き（秋）」て、歌人が「泣（鳴）」いている「失恋」の話であろうと推測される。このように「秋」から読み取る「飽きる恋」の情報は、原文の文脈（周辺語）では非明示的、つまりノンリテラルであるため、語の文字通りの意味だけでは読み取れない。

### 現代語訳の追加要素に基づく歌語の非明示的要素の可視化

以上のような非明示的要素の可視化システムの開発は、筆者が所属する山元研究室がこれまでに、現代語訳との相対化に基づく可視化システムを検討してきた [@Yamamoto2005Mathematical; @Yamamoto2006Extraction; @chin2022Tango; @Chen2024Translationbased]。

現代語訳との相対化というのは、現代語の要素の集合から和歌の原文の要素を取り除き現代語にしか存在しない要素を引き立てることである。この考えが @kondo2001Ngram, @kondo2011Heian では「引き算」と称している。しかし、引き算で残った要素、つまり、現代語訳で追加された要素はどのような性格であるか、明確にする必要がある。

翻訳における要素の追加などの処理と操作に関して、@Koller2004Einfuehrung [p. 249] では翻訳者による「介入 (Eingriff)」として、「無害」とされる介入には、目的言語の読者の不足している背景知識や、デノテーション・コノテーション面の情報、そして、言語内的、社会文化的、間テキスト的な情報の損失を補うための注釈付き翻訳手法の結果としての追加が含まれていると指摘している。翻訳が注釈・辞書同様に、解釈資料として非明示的な情報の抽出に有効な材料と認識できる。

ただし、翻訳における追加は読者の理解力を過大評価・過小評価した産物にもありうる [@Nida1964Science, p. 155; @Koller2004Einfuehrung, pp249--250]。この点においては、翻訳の追加の多様性が伺える。
古今和歌集の歌語の解釈資料としての現代語訳については、@Chen2024Translationbased では触れたものの、その多様性について十分説明していない。10 種類の現代語のコンテキストや、翻訳者の翻訳観を踏まえ再検討する余地がある。

## 本稿の意義

現代語訳の作成者はいずれも、和歌の原文に対する理解や解釈に独自の視点を持っており、それぞれ比較することによって、和歌の現代語訳の追加要素の性格のカテゴリ化、および、その普遍性を明確できる。

前述にあったように、翻訳における追加は読者の理解力の過大評価・過小評価に基づき生成した産物である可能性があるため、異なる訳者において和歌の情報をどこまで現代読者に開示すべきか、あるいは開示できるか、それぞれ打算があるはずである。訳者の意識を整理し、その異同を明確にする、そして、彼らの実践としての翻訳にはそれぞれ意識している方針を守っているかを客観的に分析することで、翻訳の普遍的な課題と限界を考察することができよう。

最終的に以上の知見を踏まえ、非明示的要素の可視化システムの根拠をより明確にでき、その改善の方向性をも明確できる。

## 問題意識と本稿の構成

非明示的要素の可視化の前提として、4 つの面から再検討すべき余地がある：

1. 注釈書における現代語訳の翻訳観の確認
2. 現代語訳において原文の要素が削りが最小限であることの証明
3. 現代語訳における追加要素が翻訳観を問わず十分であることの証明
4. 現代語訳におおける追加要素がノンリテラル要素の可視化に適していることの証明

そこで、本稿では、古今和歌集の現代語訳の歴史的変遷を辿り、古今集の現代語訳の訳者のそれぞれの翻訳観を提示し、3つの問題意識に対し考察を行う：

1. 訳者の翻訳観はどのように分類できるか。
2. 原文の要素の削除率がどれほどか。
2. 原文との一致率がどれほどあるか、翻訳観による統計的な差があるか
3. 意味の拡張単位（後述）の観点から、訳者の追加に反映されている要素は何か

本稿の構成は、以下となる：

1. 材料：本稿の資料として、テキストの歴史的位置づけと、データ量について説明する。
2. 方法
  - 翻訳方針の記述の精査による翻訳観の分類
  - 原文との差分による追加率の計算
  - 拡張意味単位モデルの観点からのケーススタディ
3. 結果
4. 考察
5. 結論

# データとその位置づけ：和歌、古今集、注釈、現代語訳

本節では、材料である古今和歌集の説明に併せ、和歌、古今集、注釈と、その現代語訳について概観する。

## 和歌

和歌は「歌」「詠む」と言われるだけあって、本来は、宮廷で、声を上げて歌われていたものであるから、基本的に当時の話しことばである。
古今集はその名前が示すように、それ以前の古代の歌と当時の歌を集めたものである。
古代の和歌は、主に『万葉集』に収められた和歌を指し、7世紀から8世紀の和歌も収録され、山部赤人、柿本人麻呂、額田王など和歌の歴史上重要な歌も仮名で収められている。

## 古今集

古今集は、その後の勅撰（天皇命令による）和歌集、二十一代集の冒頭に成立し、その後の勅撰和歌集の基本的な形式を確立したものである。
その序章には、編者、紀貫之による古今和歌集仮名序が記載されており、和歌の理論書として位置付けられている。
和歌文学の研究資料として、のちの本歌取りの元歌の前提とされている、「源氏物語」、「土佐日記」、「伊勢物語」ほか、数々の作品に掲載されている、など日本文学の基本的な資料としても重要である。
また、前述のように、話しことばの特徴を持つため、日本語の歴史的変遷を知る上でも重要な資料となっている。

## 注釈と翻訳

日本古典文学の歴史において、古今集は数多くの注釈書を生み出し、その注釈書はそれ自体の歴史ができるほどである [@kubota1960Kokin, 319]。
注釈書に見られる多くの現代語訳は読者の和歌の理解を助けるために書かれており、翻訳家の翻訳目的は主に和歌の解釈を伝えるためである。
近代については、必ずしも現代語とは言えないが、無視するわけにはいかないので、近代までの古今集の注釈の歴史を簡略に説明し、明治以後、注釈書掲載の現代語訳について考察する。

近代 (1868年) 以前は多くの古今集の注釈書が出版された。近代前（1600年--1868年）の間に、70以上の注釈書が出版されている [@kojima1989Kokin, 447--450]。表 @tbl-annotation は近代前の著名な注釈書名を５つ示す。

| 著者           | 成立年 | 注釈書名                |
| -------------- | ------ | ----------------------- |
| 北村季吟       | 1682   | 八代集抄               |
| 契沖           | 1692   | 古今和歌集余材抄       |
| 賀茂真淵       | 1784   | 古今和歌集打聴         |
| 本居宣長       | 1793?  | 古今集遠鏡             |
| 香川景樹       | 1832   | 古今和歌集正義         |

:近代前（1600--1868）の古今集の代表的な注釈書 {#tbl-annotation}

北村季吟の八代集抄は108巻（50冊）からなる八代集^[八代集とは古今集、後撰集、拾遺集、後拾遺集、金葉集、詞花集、千載集、新古今集の8つの勅撰集である。]の注釈書である。

契沖（1640--1701）の古今和歌集余材抄は最初の実践的な古今集の研究書である[@ozawa1971Kikon, p. 36]。賀茂真淵（1697--1767）による古今和歌集打聴は基本的に契沖のものを継承したものである。しかし、賀茂真淵は新しい歌よりも万葉集のような古今以前の歌に焦点を当てている。

本居宣長の古今和歌集遠鏡はすべての和歌を当時の口語に翻訳する試みである^[しかしながら、本居の翻訳意図は他の現代語の翻訳者のものとは少々異なっている。本居の意図は、古今集の注釈としてではなくて、純粋に人々に古今集がおもしろく価値のある文学であることを伝えようとしたところにあり、このことから本研究においてこの翻訳を他の翻訳者と同等に扱うことはできない。さらに、@shiozawa [1993] によると、遠鏡で使われている口語表現の実態、すなわち、どんな単語が使われているのか、どういう意味なのかがはっきりわかっているわけではないし、江戸時代の口語表現それ自身についても明らかになっているわけではないので、本研究で取り扱うことはできない。]。香川景樹（1768--1843）は「古今和歌集正義」でそれ以前の注釈書をかなり強い調子で批判している [@ozawa1971Kikon, p. 36; @matsuda1968Shinshaku, p. 58]。とはいえ、1868年以来の多くの注釈書は上記の注釈書の影響を多かれ少なかれ受けていることは確かである。

現代になって正確にはわからないが、重版、改訂などを含めて古今集の注釈書は30あまり出版されているようである。

以下に提示したように、古今集の注釈書、現代語訳に関連する著作は多数出版されている：

1. 古今和歌集 岩波文庫 佐伯 梅友 -- 1981/1/16
2. 新潮日本古典集成〈新装版〉 古今和歌集 (新潮日本古典集成 新装版) 奥村 恆哉 -- 2017/12/26
3. 新版 古今和歌集 現代語訳付き (角川ソフィア文庫) 文庫 高田 祐彦 -- 2009/6/24
4. 古今和歌集 ビギナーズ・クラシックス 日本の古典 (角川ソフィア文庫 81) 中島輝賢 -- 2007/4/25
5. 古今和歌集全評釈 (上中下) (講談社学術文庫 2542) 第 1 巻 (全 3 冊): 古今和歌集全評釈 片桐 洋一 -- 2019/2/9
6. 古今和歌集 (笠間文庫 原文 & 現代語訳シリーズ) 片桐 洋一 --
7. 古今和歌集 新編日本古典文学全集 (11) 小沢 正夫、松田 成穂 -- 1994/10/25
8. 古今和歌集 新潮日本古典集成 第 19 回奥村 恒哉 -- 1978/7/1
9. 古今和歌集（全現代語訳付）窪田空穂、 水垣久 -- 2018/11/6
10. 新訳 古今和歌集水垣久 -- 2020/4/2
11. 新日本古典文学大系 5 古今和歌集小島 憲之、 新井 栄蔵 -- 1989/2/20
12. 和歌文学大系 5 古今和歌集久保田 淳, 高野 晴代他 -- 2021/11/20
13. 古今和歌集 1 (講談社学術文庫 432) 久曾神 昇 -- 1979/9/1
14.【復刻版】西下経一「古今和歌集」―校注と解説 (響林社文庫) 西下経一、しみじみ朗読文庫 -- 2023/7/29
15. 古今和歌集 ((全対訳 日本古典新書)) 片桐洋一 -- 1980/6/1 創英社 (三省堂書店) (1980/6/1)
16. 古今和歌集 (角川文庫 黄 46-1) 窪田 章一郎 -- 1973/1/30

## データ

2000年代に入っても、多数出版されているが、これまでの可視化システムの研究において整備できたものは、比較的近代以後に出版された古今集の現代語訳である [@tbl-CT-data]。

|     | 略   | 文献    | 底本 | トークン数  | タイプ数  | 文書数  |
|-----|------|--------|------|------------:|----------:|---------:|
| 1   | KNK  | @kaneko1933Kokin | 定家 | 42,439      | 3,356     | 1,000    |
| 2   | KBT  | @kubota1960Kokin | 定家 | 32,210      | 2,701     | 1,000    |
| 3   | MTD  | @matsuda1968Shinshaku | 定家 | 31,860      | 3,007     | 1,000    |
| 4   | OZW  | @ozawa1971Kikon | 定家 | 36,173      | 3,384     | 1,000    |
| 5   | TKOK | @takeoka1976Kokin | 定家 | 29,844      | 2,861     | 1,000    |
| 6   | OKMR | @okumura1978Kokin | 定家 | 32,321      | 3,153     | 1,000    |
| 7   | KSJ  | @kyusojin1979Kokin | 定家 | 34,050      | 2,770     | 1,000    |
| 8   | KMCY | @komachiya1982Kokin | 定家 | 30,869      | 2,692     | 1,000    |
| 9   | K&A  | @kojima1989Kokin | 定家 | 33,867      | 2,955     | 1,000    |
| 10  | KTGR | @katagiri1998Kokinhyoshaku | 定家 | 36,362      | 2,882     | 1,000    |
|     | 全体 |         |      | 339,995     | 8,252     | 10,000   |

: 古今和歌集の短歌の20世紀の現代語訳10種 {#tbl-CT-data}

残念ながら、古今集は905年頃成立と言えども、これらの注釈書から文字化されたデータは和歌の現代語訳の著作権は、もちろんのこと、校訂著作権、翻刻著作権などがあり、公にできず、それぞれの注釈書著者が古今集ついていかなる考え方に基づいて執筆したのかは、十分に述べられておらず、それら現代語訳の特徴について、詳細な分析は行なわれていない。本稿では、そのうち、著作権の消滅した @kaneko1933Kokin のデータを公開し、現代語 10 種を対象にその特徴を比較する。

## データフォーマット

# 方法

## 文献調査による翻訳観の分類

前節で提示した 20 世紀の古今集の注釈書の著者の注釈方法、現代語訳執筆方針について文献調査を行い整理する。
@Yamamoto2005Mathematical [p. 102] では、訳者の理論的考えを踏まえつつ、それぞれの現代語訳を実際に観察し、「逐語訳」「作者の意思の尊重」「字句を補う」「語順・語法を変える」「不詳」のように分類しているが、「作者の意思の尊重」は目標であり、その他は具体的な訳し方である、分類の視座が統一されていないため、本稿では、コミュニケーションの観点から翻訳の力点の置き方の分類を行う。それぞれの翻訳観に基づいて、意訳と直訳の意識についてまとめる。

## 追加率・不一致率の計算

それぞれの訳者が自身の翻訳方針にどこまで一貫して訳しているかについて、実際の歌語と訳語の一致率
で計算する。一致率の計算は、要素の順序と重複を考慮しない集合演算による方法と、要素の順序と重複アライメントによる方法はあるが、ここではアライメントによる方法を用いる。具体的な手順は @Yamamoto2005Mathematical と @Yamamoto2019Analysis に詳しいが、本稿では先行研究で曖昧な部分を含め、計算のプロセスを明確にする。

```{python}
#| label: poem-mode

from typing import List

def poem_mode(poem: str, mode: int = 2) -> List[str]:
    """
    Function to filter and output data based on the given mode.
    
    Mode 1: Original output.
    Mode 2: Filter rows where the decomposition code starts with 'A' or 'B' or 'D' and the first digit of the number part is '0'.
    Mode 3: Filter rows where the decomposition code starts with 'A' or 'C' or 'E' and the first digit of the number part is '0'.
    
    :param poem: A string block with multiple lines of input data representing the poems.
    :param mode: Mode to determine the filtering behavior:
        - 1: Original output.
        - 2: Basic sense, ignore decomposition ('A', 'B', 'D' with '0' in decomposition code).
        - 3: Basic sense, consider decomposition ('A', 'C', 'E' with '0' in decomposition code).
    :return: Filtered list of strings based on the mode.
    
    Example:
    poem_test_data = '''
    01:000001:0001 A00 BG-01-1630-01-0100 02 年 年 とし 年 とし
    01:000001:0001 A10 BG-01-1911-03-1800 02 年 年 とし 年 とし
    01:000001:0002 A00 BG-08-0061-07-0100 61 の の の の の
    01:000001:0003 A00 BG-01-1770-01-0300 02 内 内 うち 内 うち
    01:000001:0004 A00 BG-08-0061-05-0100 61 に に に に に
    01:000001:0005 A00 BG-01-1624-02-0100 02 春 春 はる 春 はる
    01:000001:0006 A00 BG-08-0065-07-0100 65 は は は は は
    01:000001:0007 A00 BG-02-1527-01-0102 47 き 来 く 来 き
    01:000001:0008 A00 BG-03-1200-02-0900 74 に ぬ ぬ に に
    01:000001:0008 A10 BG-09-0010-01-0101 74 に ぬ ぬ に に
    '''

    poem_mode(poem_test_data, mode=1)
    poem_mode(poem_test_data, mode=2)
    poem_mode(poem_test_data, mode=3)
    """
    if mode == 1:
        # If mode is 1, return the original poem string split by lines
        return poem.strip().splitlines()

    # Split the input string by lines and further split each line by whitespace for other modes
    data = [line.split() for line in poem.strip().splitlines()]
    result = []

    for row in data:
        decomposition_code = row[1]  # Second column represents the decomposition code
        first_char = decomposition_code[0]  # Get the first character of the decomposition code (A, B, C, D, or E)
        
        # Ensure the first character is one of 'A', 'B', 'C', 'D', or 'E'
        # Explanation of decomposition code first_char:
        # A: No decomposition (default)
        # B: Decomposed non-special words
        # C: Components of decomposed non-special words
        # D: Decomposed proper nouns
        # E: Components of decomposed proper nouns
        assert first_char in ['A', 'B', 'C', 'D', 'E'], f"Unexpected decomposition code: {decomposition_code}"

        # Explanation of decomposition code first_digit:
        # 0: Basic sense (primary meaning)
        # Other integers: Other senses (secondary meanings or further decompositions)
        first_digit = decomposition_code[1]  # Get the first digit of the numeric part (0 for basic sense)

        if mode == 2 and first_char in ['A', 'B', 'D'] and first_digit == '0':
            # Mode 2: Basic sense, ignore decomposition (A, B, D)
            result.append(" ".join(row))
        elif mode == 3 and first_char in ['A', 'C', 'E'] and first_digit == '0':
            # Mode 3: Basic sense, consider decomposition (A, C, E)
            result.append(" ".join(row))
    
    return result
```

```{python}
#| label: translation-mode

def translation_mode(translation: str, mode: int = 2) -> List[str]:
    """
    Function to filter and output translation data based on the given mode.
    
    Mode 1: Original output. 
        - Returns the translation data as is.
    
    Mode 2: Ambiguity set to 1, ignore decomposition.
        - Filters rows where the first column is 1 (ambiguity set to 1) and the fourth column is 0 or 1 (decomposition ignored),
          and skips the current row if the next row's fourth column is 3.
    
    Mode 3: Ambiguity set to 1, consider decomposition.
        - Filters rows where the first column is 1 (ambiguity set to 1) and the fourth column is 0, 2, or 3 (decomposition considered),
          and skips the current row if the next row's fourth column is 3.
    
    :param translation: A string block with multiple lines of input data, representing the translations.
    :param mode: Mode to determine the filtering behavior.
        - 1: Original output
        - 2: Ambiguity set to 1, ignore decomposition
        - 3: Ambiguity set to 1, consider decomposition
    :return: Filtered list of strings based on the mode.

    Example:
    translation_test_data = '''
    1 katagiri 0001 1 51 50 07 BG-03-1940-01-010-A 早く はやい 早い
    1 katagiri 0001 2 51 50 07 BG-03-1660-03-010-A -- はやい 早い
    1 katagiri 0001 1 65 00 00 BG-08-0065-08-010-A も も も
    1 katagiri 0001 2 65 00 00 BG-04-1130-01-200-A -- も も
    1 katagiri 0001 1 02 00 00 BG-01-1631-01-280-A 年内 ねんない 年内
    1 katagiri 0001 3 02 00 00 BG-01-1630-01-010-A -- とし 年
    1 katagiri 0001 3 02 00 00 BG-08-0071-01-010-A -- の の
    '''

    translation_mode(translation_test_data, mode=1)
    translation_mode(translation_test_data, mode=2)
    translation_mode(translation_test_data, mode=3)
    """
    if mode == 1:
        # If mode is 1, return the original translation string split by lines
        return translation.strip().splitlines()

    # Split the input string by lines and further split each line by whitespace for other modes
    data = [line.split() for line in translation.strip().splitlines()]
    result = []

    for i, row in enumerate(data):
        ambiguity = int(row[0])
        decomposition = int(row[3])

        # Check if the current row's fourth column is 1, and if the next row's fourth column is 3
        # Explanation: the next row's fourth column == 3 means the current row is a compound unit
        if decomposition == 1 and i + 1 < len(data) and int(data[i + 1][3]) == 3:
            continue  # Skip current row if the next row's fourth column is 3

        if mode == 2 and ambiguity == 1 and decomposition in [0, 1]:
            result.append(" ".join(row))  # Ambiguity set to 1, ignore decomposition
        elif mode == 3 and ambiguity == 1 and decomposition in [0, 1, 3]:
            result.append(" ".join(row))  # Ambiguity set to 1, consider decomposition
    
    return result
```

#### 旧分類語彙表番号 (旧WLSP) に基づくマッチングのカテゴリー化 {.unnumbered}

歌語と現代語訳語の「一致・不一致」は、次の式のように、二語のメタコード（旧 WLSP 番号）の Longest common subsequence [@Sankoff1972Matching; @Traum2000Generation] の長さで $U$（不一致）, $G$（グループマッチ）, $F$（フィールドマッチ）, $E$（正確マッチ） 4 つにカテゴリ化ーする。

$$
\text{match}(s,t) \in
\begin{cases}
U, & \text{if } \text{LCS}(s,t) < 10 \\
G, & \text{if } 10 \leq \text{LCS}(s,t) < 13 \\
F, & \text{if } 13 \leq \text{LCS}(s,t) < 17 \\
E, & \text{if } \text{LCS}(s,t) \geq 17 \\
\end{cases}
$$

この式では、$s$ はソーステキスト（和歌）における語のメタコードであり、$t$ はターゲットテキスト（現代語訳）における語のメタコードである。$\text{LCS}(s,t)$ は $s$, $t$ 二語のメタコードの Longest common subsequence である。以下具体の分類例である。

1. **Unmatch** ($\text{match}(s,t) \in U$; $\text{LCS}(s,t)=4$):
  - $s=$ `[BG-0]1-5520-20-0401` 梅 (plum)
  - $t=$ `[BG-0]8-0061-07-010-A` の (of, genetive case)
2. **Group match** ($\text{match}(s,t) \in G$; $\text{LCS}(s,t)=11$):
  - $s=$ `[BG-01-5520-]20-0401` 梅 (plum)
  - $t=$ `[BG-01-5520-]19-115-A` 秋萩 (autumn *Lespedeza*)
3. **Field match** ($\text{match}(s,t) \in F$; $\text{LCS}(s,t)=11$):
  - $s=$ `[BG-01-2030-01-]0300` 神 (god)
  - $t=$ `[BG-01-2030-01-]030-A` 仏 (Buddha)
4. **Exact match** ($\text{match}(s,t) \in E$; $\text{LCS}(s,t)=17$):
  - $s=$ `[BG-01-5520-20-040]1` 梅 (plum)
  - $t=$ `[BG-01-5520-20-040]-A` 梅 (plum)

```{python}
#| label: match-string

from difflib import SequenceMatcher

# Define internal LCS (Longest Common Subsequence) function
def LCS(s: str, t: str) -> int:
    """
    Calculate the length of the longest common subsequence (LCS) between two strings.
    
    This is an internal function that uses a sequence matching algorithm to determine
    the longest common subsequence between the two input strings.

    :param s: The first input string.
    :param t: The second input string.
    :return: The length of the longest common subsequence between the two strings.
    """
    seq_matcher = SequenceMatcher(None, s, t)
    match = seq_matcher.find_longest_match(0, len(s), 0, len(t))
    return match.size

# Define match category function
def match_category(s: str, t: str) -> str:
    """
    Classify the match between two strings into one of four categories based on the LCS (Longest Common Subsequence) length.
    
    The function calculates the LCS length between two strings and classifies the match into one of four categories:
    - 'U': [U]nmatch, when LCS length is less than 10.
    - 'G': [G]roup match, when LCS length is between 10 and 12.
    - 'F': [F]ield match, when LCS length is between 13 and 16.
    - 'E': [E]xact match, when LCS length is 17 or greater.

    :param s: The first input string to compare.
    :param t: The second input string to compare.
    :return: A string representing the match category ('U', 'G', 'F', or 'E') based on the LCS length.
    """
    # Calculate LCS length using the internal function
    lcs_length = LCS(s, t)

    # Categorize based on the LCS length
    if lcs_length < 10:
        return "U"
    elif 10 <= lcs_length < 13:
        return "G"
    elif 13 <= lcs_length < 17:
        return "F"
    else:
        return "E"
```

#### バッグ法による不追加率の計算 {.unnumbered}

バッグ法による追加率（現代語訳における和歌原文に対する追加要素の割合）は、2 つのテキストを 2 つの集合と見做し、それらの集合における一致語の量をもとに計算する。ソーステキスト（和歌）$S=(s_i)_{i \in \mathbb{Z}}$ とターゲットテキスト（現代語訳） $T=(t_i)_{i \in \mathbb{Z}}$ が与えられた時、2 テキストのターゲットテキストにおける追加率の計算は下記に従う。

$$
\begin{align}
\text{U}_{T}(S,T) &= 1 - \frac{1}{|T|}\text{agreement}_{S}\\
\text{agreement}_{S} &= \sum_{s_i \in S} \mathbb{I} \left( \exists t_j \in T \text{ such that } \text{match}(s_i, t_j) \notin U \right)
$$

$\text{agreement}_{S}$ は、ターゲットテキストに対しての 2 テキストの non-repetive な一致語の数である。$\frac{1}{|S|}\text{agreement}_{S}$ を一致率（バッグ法）と定義する。$|T|$ はターゲットテキストの長さを指し、$s_i$, $t_i$ はそれぞれソーステキスト、ターゲットテキストの $i$ 番目と $j$ 番目の要素を意味する。追加率は $1 - \frac{1}{|S|}\text{agreement}_{S}$ で計算する。$\mathbb{I}$ は指示関数（Indicator Function）であり、インプットが真の場合 1 を、偽の場合 0 を返す。
^[ここの一致率と類似して、計算の厳しさは$\mathbb{I}$ 内の条件で調節できる。ここでは、$E, F, G$ のいずれのレベエルでマッチできていれば、1を返すように設定している。]。

一方で、不一致率は、原文にある要素のうち、現代語訳において対応が存在しない割合として、同様な計算ができる。

$$
\begin{align}
\text{U}_{S}(S,T) &= 1 - \frac{1}{|S|}\text{agreement}_{S}
\end{align}
$$

バッグ法で計算された追加率は、統計分析に用いる（詳しくは次節参照）。
不一致率について初歩的な記述統計をまず確認する。

```{python}
#| label: score-function

# Define score function based on match category
def score(s: str, t: str) -> int:
    """
    Calculate the score based on the match category between two strings.

    The function first classifies the match category using the `match_category` function.
    It returns:
    - 0 if the match category is 'U' (Unmatch).
    - 1 if the match category is 'G' (Group match), 'F' (Field match), or 'E' (Exact match).

    :param s: The first input string to compare.
    :param t: The second input string to compare.
    :return: An integer score where 0 represents an Unmatch and 1 represents all other matches.
    """
    category = match_category(s, t)
    
    if category == "U":
        return 0
    else:
        return 1
```

```{python}
def addition_rate_bag(poem: str, translation: str, mode: int = 1) -> float:
    """
    Calculate the translation addition rate using a bag method, comparing poem data with its translation.

    This function computes the rate at which additional elements appear in the translation
    that are not directly matched in the original poem. It uses the longest common subsequence (LCS) to determine
    the match between individual elements in the poem and translation, categorizing matches as Unmatch (U), 
    Group match (G), Field match (F), or Exact match (E).

    The function operates in three modes to filter both the poem and the translation based on decomposition:
    Mode 1: Returns the original data without any filtering.
    Mode 2: Filters rows where ambiguity is set to 1, ignoring decomposition in the fourth column.
    Mode 3: Filters rows where ambiguity is set to 1, considering decomposition.

    The addition rate is defined as:
    
        Addition Rate = 1 - (Agreement between Poem and Translation)

    Agreement is computed as the ratio of matched elements between the poem and translation texts.

    :param poem: A string representing the poem data, where each line is a tokenized element of the poem.
    :param translation: A string representing the translation data, where each line is a tokenized element of the translation.
    :param mode: Integer representing the filtering mode (default is 1).
        - Mode 1: Original output without filtering (default).
        - Mode 2: Ambiguity set to 1, ignore decomposition.
        - Mode 3: Ambiguity set to 1, consider decomposition.
    
    :return: A float representing the translation addition rate as a percentage of the translation that does not have 
             a direct match in the original poem (0 to 1 scale).
    
    Example Usage:
    --------------
    poem_data = '''
    01:000001:0001 A00 BG-01-1630-01-0100 02 年 年 とし 年 とし
    01:000001:0002 A00 BG-08-0061-07-0100 61 の の の の の
    '''
    
    translation_data = '''
    1 katagiri 0001 1 51 50 07 BG-03-1940-01-010-A 早く はやい 早い
    1 katagiri 0001 2 51 50 07 BG-03-1660-03-010-A -- はやい 早い
    '''
    
    addition_rate = addition_rate_bag(poem_data, translation_data, mode=2)
    print(f"Addition Rate: {addition_rate:.2%}")
    """
    
    # Get the filtered poem and translation lines based on the mode
    poem_lines = poem_mode(poem, mode)
    translation_lines = translation_mode(translation, mode)
    
    poem_size = len(poem_lines)
    translation_size = len(translation_lines)
    
    # Calculate agreement for translation (T)
    total_score = 0
    for t in translation_lines:
        max_score = max(score(t, s) for s in poem_lines)
        total_score += max_score

    # agreement rate between poem and translation
    agreement_T = total_score / translation_size if translation_size > 0 else 0
    addition_rate = 1 - agreement_T
    
    return addition_rate
```

#### 整列法による追加率の計算 {.unnumbered}

整列法について説明する。整列法は、動的計画法に基づく。本稿では、動的計画法を用いて 2 つのシーケンス間のアライメントを計算するためのスコーア関数の具体的な式は以下の通りである。

$$
\begin{align}
\text{S}_{\text{DP}}(i, j) &=
\begin{cases}
0 & \text{if } i = 0 \text{ or } j = 0 \\
\max
\begin{cases}
\text{S}_{\text{DP}}(i-1, j) - \text{gap} \\
\text{S}_{\text{DP}}(i-1, j-1) + \text{weight}(s_i, t_j) \\
\text{S}_{\text{DP}}(i, j-1) - \text{gap}
\end{cases} & i > 0 \text{ and } j > 0
\end{cases}\\
\text{weight}(s,t) &=
\begin{cases}
-1, & \text{if } \text{match}(s,t) \in U \\
10, & \text{if } \text{match}(s,t) \in G \\
13, & \text{if } \text{match}(s,t) \in F \\
17, & \text{if } \text{match}(s,t) \in E \\
\end{cases}\\
\text{gap} = 0
\end{align}
$$

ソーステキストのシーケンス $S$ とターゲットテキストのシーケンス $T$ の部分列を用いて、位置 $i$ および $j$ までの最適なアライメントスコア $\text{S}_{\text{DP}}(i,j)$ を再帰的に計算する。weight関数 $\text{weight}(s,t)$ はLCSに準拠する。Unmatch の場合は $-1$ のペナルティを課す。gap はギャップ（空白）を挿入する際に加算されるペナルティ (gap panelty) であり、ここでは便宜上 $0.01$ と設定する。^[単純に観測からして、和歌の翻訳で遠い対応が多く存在しているので、ギャップペナルティは 0.01 と設定することで、遠くの語の対応を許容させる。] この式に基づき、プログラムは最尤なアライメントを探索する。整列した結果の例は次のようになる。ソーステキストにギャップが挿入さら、一致する語同士が配列される。

```
>|立田姫ーー手向けるーーーー神のあれば【こそ】ーーー秋の木の葉のーーー幣とーーー散るーーーーらめ  [Kokinshu 298]
>|竜田姫は、手向けをするべき神があるので、そのつかさどる秋の木の葉が、幣のように散るのであろうよ。[@kubota1960Kokin]
```

この基礎の上で、一致率と追加率を計算する。整列後の $S^{\prime}$ と $T^{\prime}$ がある^[ここでは、もちろん $|S^{\prime}| = |T^{\prime}$|]として、整列法による一致率 $\text{agreement}^{\prime}_{T}(S^{\prime},T^{\prime})$ の計算は次に示す。

$$
\begin{align}
\text{U}^{\prime}_{T}(S,T) &= 1-\frac{1}{|T|}\text{agreement}^{\prime}_{S}\\
\text{agreement}^{\prime}_{S}) &= \sum_{s_i \in S^{\prime}} \mathbb{I} \left( \exists t_j^{\prime} \in T^{\prime} \text{ such that } \text{match}(s_i^{\prime}, t_j^{\prime}) \notin U \right)
\end{align}
$$

うち、prime ($\prime$) 記号がついているものは整列後のテキストにおけるものである。動的計画法が和歌の翻訳における語順の交換などの site swap に弱いため、整列法で計算される追加率は、全体的に統計的に分析せず、ケーススターディの説明において提示する。

## 翻訳観と追加率の関係に関する統計モデリング

先行研究では概念・フィールド・同義レベルの一致率（言い換え率）、理論的・実験的追加率などの多くの指標を計算しているが、本稿では、実験的追加率をを対象に、前節の分類結果に基づきより精緻な統計分析を行う。

訳者の翻訳観が実践に移り、実際の現代語訳における追加率に影響を与えているかどうかを確認するために、統計モデリングを行った。具体的には、翻訳観（`Focus`）が追加率（`additional_rate`）^[ここでは、バッグ法によって算出された追加率を用いる。] に与える影響を検討するために、ベータ分布^[計算された応答変数は、$(0, 1)$ の区間にあり、二項分布を仮定して用いることも考えられる。ただし、二項分布を用いる場合は、現代語訳文の全語数と追加に相当する語数の情報が必要であり、本稿の計算は @yamamoto2018 の計算結果を踏まえたため、全語数について情報が不足しており、それを前提としないベータ分布を用いる。] に基づく回帰モデルを採用した。このモデルは、訳者（`translator`）と歌（`poem`）のランダム効果も考慮し、これらの要因が追加率に与える潜在的な影響を制御するものである。モデルについて、付録を参照されたい。

モデルの推定は、ベイズ推論に基づくMarkov Chain Monte Carlo (MCMC) 法を用いて実施する。具体的には、4 つのチェーンを使用し、各チェーンで 4000 回のイテレーションを行い、そのうち 1000 回をウォームアップ (burn-in) 期間として設定する。`brms`パッケージを用いてモデルを実装し、事後分布の収束性は R-hat 指標および有効サンプルサイズ (ESS) によって診断する。モデル結果では、翻訳観が追加率に与える影響を示し、事後平均、95%事後最狭信用区間 (posterior highest credible interval) 報告する。

```{bash}
#| label: data-raw

echo "Translator,PoemID,opN,opE,opF,opG,opT,opU,ctN,ctE,ctA,ctFG,ctU,ctD,Theoretical,Gap" >\
  artifacts/data.csv
  
cat ./data/calcResidual/calcResidual-bag.txt |\
  sed 's/[=:()]/ /g' |\
  awk '{print $2,$3,$5,$6,$7,$8,$9,$10,$12,$13,$14,$15,$16,$17,$18,$19}' OFS="," >>\
  artifacts/data.csv
```

```{R}
#| label: data-process
#| message: false

data <- read.csv("artifacts/data.csv") |>
  mutate(
    AdditionRate = ctU + ctD,
    UnmatchRate = opU,
    Translator = as.factor(Translator),
    Focus = case_when(
      Translator %in% c(
        "kaneko",
        "kubota",
        "katagiri"
      ) ~ "Text-focused",
      Translator %in% c(
        "okumura",
        "takeoka"
      ) ~ "Poet-focused",
      Translator %in% c(
        "ozawa",
        "kyusojin"
      ) ~ "Reader-focused",
      Translator %in% c(
        "matsuda",
        "kojimaarai",
        "komachiya"
      ) ~ "Others",
    ),
    Focus = factor(
      Focus,
      levels = c(
        "Text-focused", 
        "Poet-focused", 
        "Reader-focused", 
        "Others"
      )
    ),
    Translator = factor(
      Translator,
      levels = c(
        "kaneko",
        "kubota",
        "katagiri",
        "okumura",
        "takeoka",
        "ozawa",
        "kyusojin",
        "matsuda",
        "kojimaarai",
        "komachiya"
      )
    )
  ) |>
  select(
    Translator,
    PoemID,
    Focus,
    AdditionRate,
    UnmatchRate
  )

translator_labels <- c(
  "kaneko" = "KNK",
  "kubota" = "KBT",
  "katagiri" = "KTGR",
  "okumura" = "OKMR",
  "takeoka" = "TKOK",
  "ozawa" = "OZW",
  "kyusojin" = "KSJ",
  "matsuda" = "MTD",
  "kojimaarai" = "K&A",
  "komachiya" = "KMCY"
  )
```


```{r}
#| label: beta-model
#| cache: true
#| messge: false

# backend
options(
    mc.cores = parallel::detectCores(),
    brms.backend = "cmdstanr"
)

# Global setting
chains <- 4
iter <- 2000
warmup <- 1000
bayes_seed <- 1234

# Formula
formula <- bf(
  AdditionRate ~ a + b, 
  a ~ 1 + (1 | Translator) + (1 | PoemID),
  b ~ 0 + Focus,
  phi ~ 1 + (1 | PoemID),
  nl = TRUE
)

prior = c(
  prior(student_t(3, 0, 2.5), nlpar = b),
  prior(
    student_t(3, 0, 2.5),
    class = b,
    coef = Intercept,
    nlpar = a
  ),
  prior(
    student_t(3, 0, 2.5),
    class = sd,
    group = Translator,
    nlpar = a
  ),
  prior(
    student_t(3, 0, 2.5),
    class = sd,
    group = PoemID,
    nlpar = a
  ),
  prior(
    student_t(3, 0, 2.5),
    class = Intercept,
    dpar = phi
  ),
  # Default prior for standard deviation of phi parameter in PoemID group
  prior(
    student_t(3, 0, 2.5),
    class = sd,
    dpar = phi,
    group = PoemID
  )
)

# Model
model <- data %>%
  brm(
    data = .,
    formula = formula,
    family = Beta(), 
    prior = prior, 
    chains = chains,
    iter = iter,
    warmup = warmup,
    seed = bayes_seed,
    silent = 2,
    adapt_delta = 0.9,
    control = list(max_treedepth = 12),
    file = "./artifacts/model_beta_bayes",
    save_model = TRUE
  )
```

## 拡張意味単位モデル (extended unit of meaning)

和歌に対する現代語訳は、なんらかの言い換えと追加を行っていることが分っても、その追加要素や言い換え要素の性格がわかっているわけではない。

これら要素の性格は、コーパス言語学の視点から分析する。本稿では @Sinclair1996Search の拡張意味単位モデルの視座を採用し、和歌の原文に観測されている拡意味単位モデルが現代語訳でどのように処理されているか、辞書の記述とどのように異なるかを確認する。

### 拡張意味単位モデル

意味の拡張単位モデルでは、意味の単位を以下の五つの要素に分けている [@Sinclair1996Search]。 @Sinclair1996Search は *true feeling* から「(not) [EXPRESS] [one's] *true feeling*」といった意味の拡張単位を抽出している例とりあげているので、この例で要素について @tbl-EUoM にまとめ説明する:

| 要素                | 説明                                                    | 例                                                                                          | 方法                                |
|:--------------------|:--------------------------------------------------------|:--------------------------------------------------------------------------------------------|:------------------------------------|
| コア（core）        | 拡張意味単位における不変の成分                            | *true feeling*                                                                              | 検索語・フレーズとして使用。         |
| コロケーション（collocation） | コアと語の共出現                                      | 高頻度共出現語 (*their*, *express*, *hide*, *reveal*) + *true feeling*                      | span $\pm$ 4、出現率50%以上。       |
| 類連結（colligation）     | コアとグラマティカル・パターンとの共出現                | 所有格形容詞 (*thier/his/her/our*) + *true feeling*                                        | コンコーダンスによる観測・帰納。    |
| 意味的志向（semantic preference） | コアと特定の意味の語群との共出現                        | **(not)EXPRESS** (*express/hide/reveal/conceal*) + *true feeling*                          | コンコーダンスによる観測・帰納。    |
| 談話韻律（discourse prosody）    | 拡張意味単位全体の評価・態度・語用論的意味               | 否定語 + **EXPRESS** -> 「客観的・主観的できない」（reluctance/inability）                        | コンコーダンスによる観測・帰納。    |

: 拡張意味単位の要素、説明、事例、操作例 {#tbl-EUoM}

1. コア（core）は拡張意味単位における不変の成分である [@Sinclair2004Trust, p. 204]。*true feeling* がこの例におけるコアであり、検索語として使用されている。^[@Sinclair1996Search の例では、コアは実際には検索語に相当するものである。]
2. コロケーション（collocation）：コアと語の共出現関係である [@Sinclair1970English, p. 15]。*true feeling* のコンテキストでは、*their*, *express*, *hide*, *reveal* などがあげられる。@Sinclair1996Search では、コンテキストのウィンドーは 4 に、閾値は 50% の出現率に設定されている。^[@Koller2004Computer はにすべきであるとしている。]
3. 類連結（colligation）はコアとグラマティカル・パターンとの共出現関係である [cf., @Firth1968Selected, p. 183; @Sinclair1996Search, p. 11; @Sinclair2003Reading, p. 171]。例えば、*true feeling* の前方共出現に *thier/his/her/our* が多く、それらの性質は所有格形容詞であるため、所有格形容詞が *true feeling* の類連接と認める。この類連接は直接の観測ができないため、コンコーダンス・コロケーションリストによる観測・帰納に基づく。
4. 意味的志向（semantic preference）はコアと特定の意味の語群（semantic set）との共出現関係 [@Sinclair2003Reading, p. 178]。例えば、*express/hide/reveal/conceal* など **EXPRESS** まとは **EXPRESS** の否定のグループが常に *true feeling* のコンテキストに生起している。この意味的志向も類連接同様、コンコーダンス・コロケーションリストによる観測・帰納に基づく。
5. 談話韻律（discourse prosody）は拡張意味単位全体の評価・態度・語用論的意味 [@Sinclair2004Trust, p. 174]である。**EXPRESS**の左には*less/not*などの否定が頻繁に出現するため、*true feeling* については「客観的・主観的に表現できない」（reluctance/inability）といったネガティブな談話韻律が推測される。これも同様、コンコーダンス・コロケーションリストによる観測・帰納による。

コア、コロケーションから談話韻律へと、直接な観測ができなくなり、明示的でなくなっていく [@Stubbs2001Words, pp. 87--88]。

### 本稿のアダプテーション

本稿では、和歌の性質とデータの量の問題点を踏まえいくつかのアダプテーションをする：

1. 順序の設定はしない。57577の制限の中で、語の順番の入れ替えが多く、コアを中心とする線形配列がまれであり、open frame が望ましい。
2. 出現率の閾値の設定はしない。テキストの数が少なく、大規模コーパスのようにソリッドな結果として出すよりも、傾向について重視すべき一面がある。
3. 事後に語の類型の帰納ではなく、抽出する前にアノテーションを行う。

# 結果

## 現代語訳の整理と分類

### 分類：コミュニケーションの力点の置き方

注釈書に記されている翻訳の方針に基づき、10 人の翻訳観をコミュニケーションの主体への視座の置き方に基づき、3 類に分類された：

1. テキストの字義を重視する方針： @kaneko1933Kokin; @kubota1960Kokin; @katagiri1998Kokinhyoshaku
2. 作者の意図を重視する方針： @okumura1978Kokin; @takeoka1976Kokin
3. 読者の理解を重視する方針： @ozawa1971Kikon; @kyusojin1979Kokin
4. 不明・その他の方針： @matsuda1968Shinshaku; @kojima1989Kokin; @komachiya1982Kokin

この中で、原文の字義を重視する翻訳観は、もっとも逐語訳に拘っている。作者の意図・感受を重視する・読者の理解を重視する翻訳観では、多少の語と語順の入れ替え、語句の補いを許容しているように述べている。

### 分類ごとの翻訳観の詳細

#### Text-focused {.unbumbered}

このグループの訳者は、原文に忠実に翻訳することを重視し、できるだけ語順や意味を変えずに翻訳を行う。
@kaneko1933Kokin は「逐語訳」に徹し、語を加えたり、削除することを避けており、慎重に翻訳していると述べている。
@kaneko1933Kokin は歌の意をそこねたり、調をあやまったりすることを恐れて、鏡花水月の訳法（あからさまに説明せず、ただその姿を眼前に思い浮ばせるようにする漢文の表現法）に従ひ、きわめて小心に、一字一語の出入をもゆるがせにせぬことを期したことを述べている。また、文体的には、金子の翻訳調はたとえば「くずをれる」のような古めかしいことばをわざと翻訳に入れるところなど、本居宣長の「遠鏡」に似ている。
@kubota1960Kokin は、現代語訳の方針について、逐語訳にこだわり、語順を変えたり、新しい語を加えることを避けていると述べている。
@katagiri1998Kokinhyoshaku も同様に、逐語訳を行い、和歌の省略や長大な増補を避け、可能な限り忠実に現代語訳している。

#### Poet-focused {.unnumbered}

このグループでは、逐語訳に拘らず、表面的な意味よりも、原文の意図と感受性を重視する群である。
@okumura1978Kokin [p. 7] は「口語訳は、原歌の言葉づかいや言い廻しを尊重し、一首の意味を厳密に解釈するより、基本的な作意をまず感じとってもらうことに重点をおいた」としている。原文の意図の理解を重視し、逐語訳よりも作意を優先して翻訳している。
@takeoka1976Kokin [p. 21] は、訳(口語訳)は原文の「筋」の紹介や解説の類ではなく，作者の認識のしかた・感じ方をあらゆる面より明らかにしたのち、それ現代語に存在する同じあるいは極力それに近似した認識のしかた、感じ方の表現にそっくりうつしかえることをいう。つまり、原作の逐語的に訳すのではまく、感受性の保持に重点を置きながら言い換えを積極的に用いる。具体的な現代語訳の方針としては、@takeoka1976Kokin [pp. 11--12] は、次のような独自の理論を提案している。
$$
S=\left[\left(a+b+c+\dots+n\right)\times X \right]\times Y
$$
ただし、$S$ は翻訳された文、$a, b, c, \dots, n$ のそれぞれは単語、$X$ は翻訳に含まれる要素、$Y$ は翻訳に効果的に加えられた要素である。$a,b,c, \dots, n$ それぞれの一般的普遍的意義は辞書で理解でき、それぞれの $+$ のしかたに相当するものは文法書を参照すれば了解できる」としている。@takeoka1976Kokin [p. 11] は $Y$ はふつうは 0 の値であるが、和歌にはいくつか効果的な要素が含まれる時、翻訳者は $Y$ に何らかの値、たとえば終助詞（わ、ね、よ）のようなものを付け加わえる。したがって、現代語訳作成のための重要な変数は $X$ となる。その $X$ は、語句の分割、対応する適切な現代語、語順、助詞・接続詞を含めた訳語、文の構造、話の流れ（談話）、場面の７つに分類されるとしている。このように、歌の筋より作者の意図を細部まで還元することを強調していると捉えられる。^[契沖の「古今和歌集余材抄」以来のの 7 種の注釈書の注を統合したはじめてのものである。竹岡はこの注釈書にて文学研究に分析的アプローチを組み込んでいる。竹岡は賀茂真淵、香川景樹の仕事を分析に根拠がないとして同意できないとしている一方で、契沖、本居宣長、富士谷成章 (1738–79) らの注釈を評価している。加えて、特に古典文法、語彙の観点から 7 種すべての古注間の違いについて慎重に議論している。]

#### Reader-focused {.unnumbered}

このグループの訳者は、原文に忠実であることよりも、読者の内容の理解や解釈を読者に伝えることを重視している。そのため、語順を変えたり、新しい語を加えることもある。
@ozawa1971Kikon は原作の語順・語法を変更することをいとわず、読者が理解しやすいように内容を優先した翻訳を行っている。 @ozawa1971Kikon [p. 46] は「口語訳はそれだけ独立しても意味がよくわかるように努めたので、時には原作の語順・語法を変えている」と述べており、逐語訳よりも内容の理解の方に力点が置かれている。
@kyusojin1979Kokin は、平易な口語訳を採用し、必要に応じて語句を補っており、解釈を重視した翻訳を行っている。現代語訳の方針としては「歌意は、歌句に基づく平易な口語訳としたが、必要と思われる場合は字句を補った」と述べている [@kyusojin1979Kokin, p.6]。

#### 不詳 {.unnumbered}

現代語訳の方針について、 @komachiya1982Kokin, @matsuda1968Shinshaku, @kojima1989Kokin は明確に述べていない。

うち、 @matsuda1968Shinshaku の現代語訳は他の 10 名の共同執筆者によって作られている。この注釈書冒頭のの解説には、現代語訳は今までの研究にしたがって作られたとは述べているものの、10 名の共著者の間での翻訳の共通認識、作成方針は明らかに述べられていない。また、松田は古今集の和歌について、歌人の感情についてだけでなく、選者の編集意識についても、解釈にも含めようとしていると述べている [@matsuda1968Shinshaku, p. 9]。各歌の後に、順次、「通釈」「語釈」「古注」「評」の項目を設け、解釈・鑑賞などが記されている。
@kojima1989Kokin の特色の一つは注釈は江戸期 (1600–1868) だけでなく、これまでの注釈書では無視されてきた中世期 (1392–1600) に作られた注釈も取り入れていることである (pp. 481--482)。また、万葉集の和歌と用語が対比されて引用されている (小島・新井 1989: 480)。各歌の注釈としては「歌番号」「大意」「語句の注」「参考事項」の順で述べられている。他の注釈本と比べてさほど量的に違いはないが、付録にはさまざまな資料が含まれており、付録だけで本の 30 パーセントを占めているほどである。

#### まとめ {.unnumbered}

それぞれの翻訳の方針を見る限り、ほとんどの作者が逐語訳を基本に原文の意味を変えない翻訳を試みているが、力点の置き方が若干異なっている。本稿では、「テキストの字義を重視」「作者の意図・感受の解釈を重視」「読者の理解を重視」「不詳」のようにコミュニケーションの観点から分類を行った。この中で、原文の字義を重視する翻訳観は、もっとも逐語訳に拘っている。作者の意図・感受を重視する・読者の理解を重視する翻訳観では、多少の語と語順の入れ替え、語句の補いを許容している。ただし、この分類は、もちろんのこと、互いに重なる部分もあるが予想される。

## 追加率・不一致率の概要

追加率の計算結果は、@fig-data を参照する。

```{R}
#| label: fig-data
#| fig-scap: 訳者別の追加率の確率分布
#| fig-cap: 訳者追加率の確率分布
#| messge: false
#| warning: false

by_focus_annotation <- data |> group_by(Focus) |> 
  get_summary_stats(
    AdditionRate, 
    type = "mean_sd"
  ) |>
  mutate(
    annotation = paste0(
      formatC(mean * 100, format = "f", digits = 1),
      "*'%'~(N==",
      n,
      "*';'~std.==", 
      formatC(sd, format = "f", digits = 3),
      ")"
    ) 
  ) |>
  select(annotation) |>
  pull()

by_translator_annotation <- data |> group_by(Translator) |> 
  get_summary_stats(
      AdditionRate, 
      type = "mean_sd"
  ) |>
  mutate(
    annotation = paste0(
      formatC(mean * 100, format = "f", digits = 1),
      "*'%'~(N==",
      n,
      "*';'~std.==", 
      formatC(sd, format = "f", digits = 3),
      ")"
    )
  )|>
  select(annotation) |>
  pull()

data |>
  arrange(Focus, Translator) |>
  ggplot(
    aes(
      x = AdditionRate, 
      y = Translator, 
      fill = Focus,
      )
    ) + 
  stat_density_ridges(
    color = palette_okabe_ito(5),
    quantile_lines = TRUE, 
    quantiles = c(0.025, 0.5, 0.975), 
    jittered_points = TRUE, 
    position = "raincloud",
    vline_width = 1, vline_color = "red",
    point_size = 0.001, 
    point_alpha = 0.1,
    point_color = "black",
    alpha = 0.8,
    scale = 1,
    ) +
  # scale_fill_brewer(palette = "Blues") +  
  scale_x_continuous(
    labels = label_percent(),
    breaks = c(0, 0.25, 0.5, 0.75, 1),
    limits = c(-0.2, 1.2)
  ) +  
  scale_y_discrete(labels = translator_labels) +
  geom_hline(
    yintercept = c("matsuda", "ozawa", "okumura", "kaneko"),
    linetype = "solid", 
    color = "black",
    linewidth = 1.5
    ) + 
  annotate(
    geom="text",
    x = -0.2, y = c("kojimaarai", "kyusojin", "takeoka", "katagiri"),
    # color = "blue",
    color = palette_okabe_ito(order=c(2, 3, 6, 1)),
    vjust = -0.1,
    hjust = 0,
    size = 6,
    label = c(
      "Others", 
      "Reader-focused",
      "Poet-focused",
      "Text-focused"
      )
    ) + 
  annotate(
    geom="text",
    x = -0.2, y = c("kojimaarai", "kyusojin", "takeoka", "katagiri"),
    color = palette_okabe_ito(order=c(2, 3, 6, 1)),
    vjust = 1.2,
    hjust = 0,
    size = 3,
    label = by_focus_annotation,
    parse = TRUE
    ) + 
  annotate(
    geom="text",
    x = 1.2, y = c(levels(data$Translator)),
    color = "black",
    vjust = -0.2,
    hjust = 1,
    size = 3,
    label = by_translator_annotation,
    parse = TRUE
    ) +
  xlab("Addition Rate") +
  scale_fill_okabe_ito(order = c(1, 6, 3, 2), guide = "none") +
  labs(
    fill = "Translation Focus: ",
    caption = "2.5%, 50%, 97.5% quantiles are shown with red line"
  ) + 
  theme_set_b() +
  theme(
    axis.text.x = element_text(
      angle = 0, 
      hjust = 0.5, 
      vjust = 0.5
    ),
    axis.text.y = element_text(
      face = "italic"
    ),
    legend.title = element_text(size = 12),
    axis.title.y = element_blank()
  ) +
  guides(fill = guide_legend(nrow = 2)) 
```

```{r}
#| label: fig-unmatch-rate
#| fig-scap: 不一致率（明確な対応をもたない要素が和歌原文を占める割合）の概要
#| fig-cap: 不一致率（明確な対応をもたない要素が和歌原文を占める割合）の概要

by_focus_annotation <- data |> group_by(Focus) |> 
  get_summary_stats(
    UnmatchRate, 
    type = "mean_sd"
  ) |>
  mutate(
    annotation = paste0(
      formatC(mean * 100, format = "f", digits = 1),
      "*'%'~(N==",
      n,
      "*';'~std.==", 
      formatC(sd, format = "f", digits = 3),
      ")"
    ) 
  ) |>
  select(annotation) |>
  pull()

by_translator_annotation <- data |> group_by(Translator) |> 
  get_summary_stats(
      UnmatchRate, 
      type = "mean_sd"
  ) |>
  mutate(
    annotation = paste0(
      formatC(mean * 100, format = "f", digits = 1),
      "*'%'~(N==",
      n,
      "*';'~std.==", 
      formatC(sd, format = "f", digits = 3),
      ")"
    )
  ) |>
  select(annotation) |>
  pull()

data |>
  arrange(Focus, Translator) |>
  ggplot(
    aes(
      x = UnmatchRate, 
      y = Translator, 
      fill = Focus,
    )
  ) + 
  stat_density_ridges(
    color = palette_okabe_ito(5),
    quantile_lines = TRUE, 
    quantiles = c(0.025, 0.5, 0.975), 
    jittered_points = TRUE, 
    position = "raincloud",
    vline_width = 1, vline_color = "red",
    point_size = 0.001, 
    point_alpha = 0.1,
    point_color = "black",
    alpha = 0.8,
    scale = 1,
  ) +
  # scale_fill_brewer(palette = "Blues") +  
  scale_x_continuous(
    labels = label_percent(),
    breaks = c(0, 0.25, 0.5, 0.75, 1),
    limits = c(-0.2, 1.2)
  ) + 
  scale_y_discrete(labels = translator_labels) +
  geom_hline(
    yintercept = c("matsuda", "ozawa", "okumura", "kaneko"),
    linetype = "solid", 
    color = "black",
    linewidth = 1.5
  ) + 
  annotate(
    geom = "text",
    x = 0.4,
    y = c(
      "kojimaarai", "kyusojin",
      "takeoka", "katagiri"
    ), 
    # color = "blue",
    color = palette_okabe_ito(order=c(2, 3, 6, 1)),
    vjust = -0.1,
    hjust = 0,
    size = 5,
    label = c(
      "Others", 
      "Reader-focused",
      "Poet-focused",
      "Text-focused"
    )
  ) + 
  annotate(
    geom = "text",
    x = 0.4, y = c("kojimaarai", "kyusojin", "takeoka", "katagiri"),
    color = palette_okabe_ito(order=c(2, 3, 6, 1)),
    vjust = 1.2,
    hjust = 0,
    size = 2.5,
    label = by_focus_annotation,
    parse = TRUE
  ) + 
  annotate(
    geom="text",
    x = 1.2, y = c(levels(data$Translator)),
    color = "black",
    vjust = -0.2,
    hjust = 1,
    size = 2.5,
    label = by_translator_annotation,
    parse = TRUE
  ) +
  geom_vline(
    xintercept = 0.2,
    color = palette_okabe_ito(9),
    linewidth = 1
  ) +
  xlab("Addition Rate") +
  scale_fill_okabe_ito(order = c(1, 6, 3, 2), guide = "none") +
  labs(
    fill = "Translation Focus: ",
    caption = "2.5%, 50%, 97.5% quantiles are shown with red line\n20% line is shown with black dashed line"
  ) + 
  theme_set_b() +
  theme(
    axis.text.x = element_text(
      angle = 0, 
      hjust = 0.5, 
      vjust = 0.5
    ),
    axis.text.y = element_text(
      face = "italic"
    ),
    legend.title = element_text(size = 12),
    axis.title.y = element_blank()
  ) +
  guides(fill = guide_legend(nrow = 2)) 
```
## 統計モデルの推定結果

### 翻訳観による追加率の相違が小さい

```{r}
#| label: posterier-data
#| messge: false
#| warning: false

set.seed(1234)

pred_addition_rate <- model |> 
  epred_draws(
    newdata = tibble(
      Focus = c(
       'Text-focused', 
       'Poet-focused',
       'Reader-focused',
       'Others'
      ),
      PoemID = NA,
      Translator = NA
    )
  ) |>
  mutate(
    Focus = factor(
      Focus,
      levels = c(
        "Text-focused", 
        "Poet-focused", 
        "Reader-focused", 
        "Others"
      )
    )
  )

pred_focus_annotation <- pred_addition_rate |>
  median_hdi(.epred, .width = 0.95) |> 
  mutate_if(is.numeric, round, digits = 2) |>
  mutate(median = paste0(
    "$",
    .epred*100,
    "\\%$"
    )
  ) |>
  mutate(annotation = paste0(
    .epred, 
    "*'%; '*",
    .width * 100,
    "*'% CrI ['*", 
    .lower, 
    "*', '*",
    .upper, 
    "*']'", 
    sep = ""
    )
  ) |>
  mutate(description = paste0(
    "$\\mathrm{posterior\\; median} = ",
    .epred, 
    "\\%; ",
    .width * 100,
    "\\%\\;\\mathrm{CrI} = [", 
    .lower, 
    ", ",
    .upper, 
    "]$", 
    sep = ""
    )
  ) |>
  select(Focus,annotation, description, median)

pred_addition_rate_diff <- pred_addition_rate |> 
  compare_levels(
    variable = .epred, 
    by = Focus
  ) |>
  mutate(
    Focus = factor(
      Focus,
      levels = c(
        "Poet-focused - Text-focused",
        "Reader-focused - Poet-focused",
        "Reader-focused - Text-focused",
        "Others - Poet-focused",
        "Others - Reader-focused",
        "Others - Text-focused"
      )
    )
  )

pred_diff_CrI_annotation <- pred_addition_rate_diff |>
  median_hdi(.width = 0.95) |> 
  mutate_if(is.numeric, round, digits = 2) |>
  mutate(median = paste0(
    "$",
    .epred*100,
    "\\%$"
    ) 
  ) |>
  mutate(annotation = paste0(
    "Delta==",
    .epred, 
    "*'%; '*",
    .width * 100,
    "*'% CrI ['*", 
    .lower,  
    "*', '*",
    .upper, 
    "*']'", 
    sep = ""
    )
  ) |>
  mutate(description = paste0(
    "$\\mathrm{posterior\\; median} = ",
    .epred, 
    "\\%; ",
    .width * 100,
    "\\%\\;\\mathrm{CrI} = [", 
    .lower, 
    ", ",
    .upper, 
    "]$", 
    sep = ""
    )
  ) |>
  select(Focus, annotation, description, median)

pred_diff_prob_g_0_annotation <- pred_addition_rate_diff |>
  group_by(Focus) |>
  summarise(
    prob_g_0 = mean(.epred > 0) * 100
  ) |>
  mutate_if(is.numeric, round, digits = 1) |>
  mutate(annotation = paste0(
    "italic(P)(Delta>0)==", 
    prob_g_0,
    "*'%'",
    sep = ""
    )
  ) |>
  mutate(description = paste0(
    "$P(\\Delta>0) = ",
    prob_g_0,
    "\\%$",
    sep = ""
    )
  ) |>
  select(Focus, annotation, description, prob_g_0)
```

モデルの事後分布から翻訳観ごとの現代語訳の追加率をサンプリングした [@fig-poster]。モデルの可視化と解釈について、@Yu2020Tradeoff を参考に、比較 2 群の差の事後分布の 95% の CrI が 0 をカバーしているかを観測する以外に、2 群の差が 0 より大きい確率を同時に確認する。まず、95% の CrI が 0 をカバーしていない場合、2 群に差があると判断する；95% の CrI が 0 をカバーしていても、2 群の差が 0 より大きい確率が 95% より大きい、または 5% より小さい場合、傾向差があると判断する。

(a) 追加率は、翻訳観を問わず 現代語訳の要素の 50% 前後が、追加的な要素であることがわかった。この点は、観測データからも確認されていた。
(b) 翻訳観グループで比較した結果、2 グループの差の事後分布の 95% の CrI はすべて 0 をカバーしており、それぞれの 2 群の追加率は異なることが認められなかった。
傾向としては、`Reader-focused` 群の追加率に比べ `Other` 群の追加率が `{r} pred_diff_CrI_annotation |> filter(Focus == "Others - Reader-focused") |> pull(median) |> I()` 程度下回る (`{r} pred_diff_CrI_annotation |> filter(Focus == "Others - Reader-focused") |> pull(description) |> I()`, `{r} pred_diff_prob_g_0_annotation |> filter(Focus == "Others - Reader-focused") |> pull(description) |> I()`)。`Reader-focused` 群の追加率は、`Poet-focused` 群より `{r} pred_diff_CrI_annotation |> filter(Focus == "Reader-focused - Poet-focused") |> pull(median) |> I()`% 程度下回る (`{r} pred_diff_CrI_annotation |> filter(Focus == "Reader-focused - Poet-focused") |> pull(description) |> I()`, `{r} pred_diff_prob_g_0_annotation |> filter(Focus == "Reader-focused - Poet-focused") |> pull(description) |> I()`) ことが観測されているが、傾向差の基準を満していない。

```{r}
#| label: fig-poster
#| fig-scap: 予測された追加率の事後分布
#| fig-cap: 予測された追加率の事後分布
#| fig-subcap: 
#|   - "予測された各翻訳観の追加率の事後分布"
#|   - "予測された翻訳観による追加率の相違の事後分布"
#| messge: false
#| warning: false
#| laylayout-nrow: 2
##| layout: [[61, 27]]

pred_addition_rate |>
  ggplot(aes(x = .epred, y = Focus)) + 
  stat_slab(
    aes(
      fill = Focus,
      fill_ramp = after_stat(
        cut_cdf_qi(cdf, .width = c(0.02, 0.8, 0.95, 1))
        )
      ),
    # height = 4,
    color = "white",
    slab_size = 0.05,
  ) + 
  stat_pointinterval(
    .width = c(.95, .8, .2)
  ) + 
  scale_fill_okabe_ito(order = c(1, 6, 3, 2), guide = "none") +
  scale_fill_ramp_discrete(range = c(1, 0.2), guide = "none") +
  scale_x_continuous(
    labels = label_percent(),
    breaks = c(0, 0.25, 0.5, 0.75, 1),
    limits = c(-0.4, 0.85)
    ) +  
  labs(
    x = "Estimated Addition Rate",
    y = "Focus",
    caption = "Posterior medians are shown with points\n 80% and 95% credible intervals (HDI) are shown with inner, outer intervals and shading"
  ) +
  annotate(
    geom="text",
    x =-0.4, y = c(levels(pred_addition_rate$Focus)),
    color = palette_okabe_ito(order=c(1, 6, 3, 2)),
    # color = "black",
    vjust = -0.2,
    hjust = 0,
    size = 5,
    label = pred_focus_annotation$annotation,
    parse = TRUE
  ) + 
  geom_vline(
    xintercept = 0.5, 
    color = palette_okabe_ito(9), 
    linetype="dashed",
    linewidth = 1
  ) +
  theme_set_b() +
  theme(
    axis.text.x = element_text(
      angle = 0, 
      hjust = 0.5, 
      vjust = 0.5
    ),
    axis.title.y = element_blank()
  )

pred_addition_rate_diff  |>
  separate(
    Focus, 
    into = c("FocusA", "FocusB"), 
    sep = " - ",
    remove = FALSE
  ) |>
  mutate(
    FocusA = factor(
      FocusA,
      levels = c(
        "Text-focused", 
        "Poet-focused", 
        "Reader-focused", 
        "Others"
      )
    ),
    FocusB = factor(
      FocusB,
      levels = c(
        "Text-focused", 
        "Poet-focused", 
        "Reader-focused", 
        "Others"
      )
    )
  ) |>
  ggplot(
    aes(
      x = .epred, y = Focus
    )
  ) + 
  stat_halfeye(
    aes(
      fill = FocusA, 
      fill_ramp = stat(x < 0)
    )
  ) +
  stat_pointinterval(
    .width = c(.95, .8, .2)
  ) + 
  scale_fill_okabe_ito(order = c(6, 3, 2)) +
  scale_fill_ramp_discrete(
    from = "grey95",
    range = c(1, 0), guide = "none") +
  scale_x_continuous(
    labels = label_percent(),
    breaks = c(-0.25, 0, 0.25, 0.5, 0.75),
    limits = c(-0.6, .4)
    )+  
  labs(
    x = "Estimated Addition Rate Differences",
    y = "Focus",
    caption = "Posterior medians are shown with points\n 80% and 95% credible intervals (HDI) are shown with inner and outer intervals",
    fill = "Base Translation Focus"
  ) +
  annotate(
    geom="text",
    x = -0.05, y = c(levels(pred_addition_rate_diff$Focus)),
    # color = palette_okabe_ito(order=c(1, 6, 3, 2)),
    # color = "black",
    vjust = -0.1,
    hjust = 1,
    size = 3,
    label = pred_diff_CrI_annotation$annotation,
    parse = TRUE
  ) + 
  annotate(
    geom="text",
    x = 0.1, y = c(levels(pred_addition_rate_diff$Focus)),
    color = palette_okabe_ito(order=c(6, 3, 3, 2, 2, 2)),
    # color = "black",
    vjust = -0.1,
    hjust = 0,
    size = 3,
    label = pred_diff_prob_g_0_annotation$annotation,
    parse = TRUE
  ) + 
  geom_vline(
    xintercept = 0, 
    color = palette_okabe_ito(9), 
    linetype="dashed",
    linewidth = 1
  ) +
  theme_set_b() +
  theme(
    axis.text.x = element_text(
      angle = 0, 
      hjust = 0.5, 
      vjust = 0.5
    ),
    axis.title.y = element_blank()
  ) +
  guides(fill = guide_legend(nrow = 3)) 
```

### 訳者による追加率の変動より歌による変動のほうが大きい

階層モデリングにおいて、訳者と歌をランダム切片としてモデル化した。各グループレベルがそれぞれ共有していると仮定した正規分布のパラメータである標準偏差の事後分布を可視化した [@fig-hyperparameter]。訳者グループの標準偏差の分布が歌グループのより左寄りであることが確認された。訳者の間の変動よりも、歌の間の変動のほうが大きいことが伺える。つまり、ランダム効果の視点からは、歌が訳者に比べ追加率の変動に寄与していると考えられる。

```{r}
#| label: fig-hyperparameter
#| fig-scap: グループレベルのハイパーパラメータの事後分布
#| fig-cap: グループレベルのハイパーパラメータの事後分布
#| warning: false
#| message: false

post <- model |> 
  as_draws_df() |>
  select(starts_with("sd")) |>
  select(-contains("pattern"))

group_labels <- c(
  "sd_PoemID__a_Intercept" = "Poem",
  "sd_Translator__a_Intercept" = "Translator"
  )

post |>
  pivot_longer(sd_PoemID__a_Intercept:sd_Translator__a_Intercept) |> 
  mutate(name = factor(name)) |>
  ggplot(aes(x = value, fill = name)) +
  geom_density(linewidth = 0, alpha = 3/4, adjust = 2/3, show.legend = F) +
  annotate(
    geom = "text", 
    x = 0.3, y = 20, 
    label = expression(sigma["Poem"]),
    color = palette_okabe_ito(7)
  ) +
  annotate(
    geom = "text", 
    x = 0.1, y = 10, 
    label = expression(sigma["Translator"]), 
    color = palette_okabe_ito(5)
  ) +
  scale_fill_okabe_ito(order = c(7, 5), guide = "none") + 
  scale_x_continuous(
    breaks = c(0.1, 0.2, 0.3),
    limits = c(0, .4)
    )+  
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    x = expression("Group hyperparameter"~sigma~"value"),
    y = "Group",
  ) +
  theme_set_b() +
  theme(
    axis.text.x = element_text(
      angle = 0, 
      hjust = 0.5, 
      vjust = 1
    ),
    axis.title.y = element_blank(),
    panel.grid.major.x = element_line(
      color = "gray80",
      linetype = "solid"
    )
  )
```

## 拡張意味単位の対応：「立田」の場合

### 「立田」の拡張意味単位：「立田+自然物+余韻結び」

#### コロケーション

#### コリゲーション

#### 意味的志向

#### 談話韻律

### 辞書の対応

### 現代語訳の対応

# 考察

## 訳者の翻訳観はコミュニケーションモデルにおけるハイライトの違いで解釈できる


訳者の 4 つの翻訳観は、それぞれ @Schramm1954Process のコミュニケーションモデルにおいて力点の置き方で解釈できる (@fig-schramm-schema)。

::: {#fig-schramm-schema layout-nrow=2}

![コミュニケーションモデル](figures/fig-process-comm.svg){#fig-schramm-schema-orig}

![翻訳観の分類](figures/fig-schema-op-ct-tikz.svg){#fig-schramm-schema-adap}

コミュニケーションモデルから見る翻訳観の分類
:::

現代語訳のコミュニケーションモデルは 2 つのサブプロセスを含めている。翻訳者がサブプロセス 1 の受信者でありながら、サブプロセス 2 の発信者である。2 つのサブプロセスの経験野をコミュニケートする役割を果たしている。

このモデルの考え方では、

1. 歌人の作意を重視することは、コミュニケーションモデルのサブプロセス（10 世紀）の中のソースをハイライトすることである
2. テキストを重視することは、コミュニケーションのサブプロセス（10 世紀）におけるシグナルであるテキストのリテラルな意味をハイライトしている
3. 読者にとっての読みやすさを重視することは、コミュニケーションモデルのサブプロセス（20 世紀）の中のデスティネーションをハイライトすることである。

現代語訳からノンリテラル要素を抽出する場合、翻訳観 3 の訳文からノンリテラル要素を多く抽出できそうであった。しかし、訳者の意識とは別に、翻訳を実践する際に、翻訳の実践と翻訳観のずれがあることがある。この点について、分析 2 では明確にしている。

## 追加率には翻訳観に左右されない部分がある

追加率について、別の角度から再計算し、素のデータに対して予測モデリングを行った結果、いずれの翻訳観においても、相当の比例の追加要素が含まれえていることが再現できた。訳者の間の摂動より、歌による摂動のほうが大きいと考えられる。注釈書では、多くの語の解釈が注釈によって詳しく解説されていながらも、追加率が低下しているとはいえない。訳者の目線がテキストを重視するにせよ、作者の意図を重視するにせよ、読者の読み易さを重視するにせよ、基本的に追加せざるをえない要素があることがあると考えられる。それらの要素は、つまり、翻訳の目線、訳者の違いに左右されないものであり、その性質を明確する余地が残っている。現代語訳によるノンリテラル要素の可視化が十分考えられる。


## 現代語訳における「欠落」の多くは序詞、枕詞

欠落されているものは、むしろなんらかの形で訳されている可能性はあり、、、、

省略率の計算では、10種の現代語訳に共通してみられた問題点は「序詞」「枕詞」「まわりくどい言い回し」の３点である。訳者の幾人かは意識的に「序詞」「枕詞」を訳さないで放置していることが観測された。
たとえば、@matsuda1968Shinshaku の現代語訳では、4 首は現代語訳の方がもとの歌より短くなっている。^[値は]
CT{\scriptsize (matu, 173)}=-0.08と
CT{\scriptsize (matu, 665)}=-0.15の訳においては、
枕詞\index{まくらことば@枕詞}は省略されている。
CT{\scriptsize (matu, 684)}=-0.08の訳においては、
CT{\scriptsize (matu, 697)}=-0.88においては、
枕詞\index{まくらことば@枕詞}も序詞\index{じょことば@序詞}も
省略されている。

これらの歌は当然不一致率も高い。

特に、CT{\scriptsize(matu, 697)}はもとの和歌よりもずっと短い\footnote{CT{\scriptsize (kyuu,404)}=-0.60もまた極端に短く、序詞がやはり
省略されている。}。
CT{\scriptsize(matu, 697)}では大和の枕詞「敷島の」が省略されている。
OP{\scriptsize(697)}は、「頃も」と「衣」が掛詞になっているが、
その「衣」に掛かっていく序詞\index{じょことば@序詞}も省略されている。

さて、和歌と現代語訳の間にある回りくどい表現についても、どうにかして一致させたいと願うかもしれないが、残念ながら、その願いを叶えることは技術的に難しい。
ある語と回りくどい言い方が同等の意味を持つことばを辞書に用意し、二者間の一致を実現させることは実に難しい。
比較的やさしい方法があるとするならば、両者の表現をもう一段下位の概念で定義して、下位の概念同士が一致するかどうかを試してみる。
うまくいってほしいと期待するしかないが、それは期待以外の何者でもない。
なぜなら、すべての語について該当する下位概念を言い当て、それを記述すること自体が難しいからだ\footnote{たとえば、「桜」「花」という上位の概念語を当てることは簡単だが、「花」という語を見て、それが「桜」であるという保証はどこにもない。} 。

上では問題点だと述べたが、一致しない要素（つまり、現代語訳のあまりもの）は、コノテーションを調べる目的からすれば、むしろ貴重なものなのである。

したがって、すべてが一致してしまうことより、一致しないものがあることの方
がこの研究にとってはおもしろく意義のあることなのである。

## 翻訳一般性仮説の観点から見る現代語訳の操作

翻訳における言い換え、追加、省略について、翻訳研究の観点からしばしば、翻訳一般性（普遍性）の仮説で解釈されている。@Chesterman2004Hypotheses と @Edina2016Translation に基づいて、翻訳において潜在的にかかわりうる翻訳一般性を以下に整理している：

- 単純化 (cf. law of simplication)：ソース言語の言語面、あるいは、情報面の翻訳における単純化 [@Baker1996Challenges]
- 標準化・慣習化 (cf. law of standardization / conventionalization)^[その逆として、ソース干渉の定理 (law of interference) が提起されている。それは、翻訳において外国語・外国文化を翻訳において伝わるようにする傾向を指す [@Tully2014Translation, p. 295]。]
  - ソース言語の外国的特徴 (foreign feature) をターゲット言語のの文化へと修正し [@Tully2014Translation, p. 295]、
  - 翻訳のプロセスにおいてターゲット言語における典型的なパターンを踏襲し、ないしはそれを強調する [@Baker1993Corpus，p. 176]
  - ソーステキストのテーマがターゲットテキストのレパートリーに変換される傾向がある [@Toury1995Descriptive, p. 268] 
- 長さ増加の定理 (law of lenghtening)：ターゲットテキストのがソーステキストより長い傾向 [@Vinay1958Comparative]
- 明示化の定理 (law of expicitation)：@Blum-Kulka1986Shifts では、翻訳者がソーステキスト にはない Cohensive marker^[According to @Nunan1993Introducing [p. 21], cohensive markers are “words and phrases which enable the writer or speaker to establish relationship across sentence or utterance boundaries, and which help to tie the sentences in a text together”.] をターゲットテキストで示すことが示唆されている。@Baker1996Challenges [p. 180] は、この問題を「翻訳では『物事を暗黙的なままにしておくのではなく、明確に説明する』傾向がある」とより広く捉えられるようになった。ただし、ここで明示化とは、翻訳文や書き下し文で使用される品詞（接続詞、副詞、関係代名詞の使用など）の追加で原文の情報を明示化することを指す [@Jia2022Myth; @Puurtinen2004Corpusbased; @Palumbo2009Key]。^[明示化の反対に、暗示化 (law of implicitation) もあり、暗示化とは、暗示のプロセスは「原文では明確に明示されている情報を暗示的にする」ことである [@Bednar2015Social, p: 3]。]
- 重複削減の定理 (reduction of repetition)：翻訳において、原文に存在する重複が減少する [@Baker1993Corpus]

本稿では、現代語訳の追加要素の存在について、長さ増加の普遍性、明示化の普遍性からしては、それが不思議なものではなく、むしろ翻訳における普遍的な現象である。
ただし、追加要素が、最終的に本稿の最終目的である歌ことばの非明示的情報の可視化につながっていくかいなかについて、単純な問題ではない。

- 明示化のルールは、語の説明的補足よりも、翻訳研究ではわかりやすくするための文法的・機能的要素の追加が注目されている。つまり、個々の歌ことばの明示化のための要素ではなく、文の意味の明示化のための要素が主眼になっている。歌ことばのノンリテラル要素の可視化においては、文法的機能的な要素が古今和歌集の翻訳の多くを占めることは望ましくないとしている。
- 実際、明示化に見えたことは、通時的な変化や、文化的なギャップである可能性もあり [@Mauranen2008Universal p. 39; @Yamamoto2019Analysis, p. 68]  、ノンリテラル要素の可視化においては、文化的ギャップを埋める明示化のほうが望ましい。
- 文体的に明瞭な表現にする、いわゆる言語の様式（言い換え）の平易化を介した明示化と、翻訳の操作で非明示的な要素の提示による明示化の区別も重要である。後者は情報の詳細化につながるため、本稿においては後者が望ましい。

ケーススタディの例では、文法的・機能的要素、様式の平易化、通時的変化でない、語の明示化の要素とみられるものも多く観測されていたため、一応、ノンリテラル要素の抽出として価値があると考えている。

一方で、翻訳における省略の現象に関連する普遍性については、単純化と、暗示化と、重複の削除^[重複の削除は、前節で説明した現象を包括しているため、前節を参照されたい。] があると思われる。
単純化というのは、

1. 翻訳において表現が欠落するか、
2. 情報が欠落するか、または
3. 語彙の豊富さ・密度が減るか、

といった 3 つの側面がある。本稿の目的とかかかるのは、表現の欠落と情報の欠落である。表現が欠落することは、必ずしも情報の欠落を意味しているというわけではないが、語の追加もまた情報の追加を意味しない。この点については、暗示化と関連している。暗示化は、つまり、情報をターゲットテキストの語彙的要素・文法的要素に隠すことであり、情報量を削らない。本稿の議論では、現代語訳と原文の情報量が同等であることを前提にしていることに注意されたい。^[古語の意味は、筆者らが不可知・不可視であると捉える立場に立ち、訳者ができれば情報の欠落なく訳していると仮定している。]
この場合、表現の追加・省略は、訳者が強調したい情報が明らかにし、重要でないと思われている情報が背景にする、いわゆる注意力の配分の問題であると考えられる。追加要素の分析の価値が見出される。ただし、本稿の前提が強めの仮説であることを認めざるをえない、欠落の情報があり暗示化することも確認されていることは、つまり可視化において背景化された情報が不可視のままを意味している。翻訳に基づくノンリテラル可視化の課題のひとつと考えられる。

翻訳の標準化・慣習化の普遍性も古今和歌集の現代語訳において、追加と省略、とりわけ言い換え操作の解釈として考えられる。したし、「古今和歌集の注釈書における現代日本語訳」というのは、極めて特殊な例であると考えられる。同言語内の通時的変種間の翻訳であるため、いわゆる外国的な性質の保留はつまり古風的に訳すことになり、しかし、注釈書にある翻訳であるため文学的に訳す必要がない。標準化・慣習化はこのケースでは、どのような現象につながるかは不明であり、翻訳の操作の内実について検討する余地がある。

このように、現代語訳における追加と省略は、一部は翻訳普遍性仮説で説明することができた。すべての現象についてこの仮説で解釈されることが難しい^[実際、その普遍性は、これらの一般性定理は、結局いずれも反例が存在するため、確率的な傾向、あるいは、普遍性とすることが妥当でであるとされてる [cf., @Pym2008Toury; @Tymoczko1998Computerized; @Chesterman2004Hypotheses; @Chesterman2010Why]。とりもなおさず、一般性といいつつ例外が認めているわけである。
]が翻訳を用いて語の非字義的な要素の可視化を目指す本稿では、翻訳普遍性の仮説から価値と課題と両方がみられた。

<!-- 仮説への批判: -->
<!-- 贝克对翻译英语语料库的研究（TEC，Laviosa，1997，1998）局限于英语作为目标语言或源语言，而忽视了作者、体裁和源语言的起源（Martin，2017；Tsai，2020）。 -->
<!-- Pym (2010，第 78 页) 认为，普遍性要求某种语言现象只出现在翻译文本中，而在其他文本中不出现。他认为，这些语言普遍性应该属于翻译文本特征的分类之一。另一方面，实证研究结果（例如Puurtinen，2004 年；Saldanha，2004 年；Becher，2011 年）指出，并非所有翻译都存在此类固有特征，因此研究人员不得不认为，所揭示的现象不可能与所有类型的文本及其翻译背景相关（Tymoczko，1998 年）。 -->
<!-- Tymoczko (1998)等人指出了相关的缺陷。每当需要生成或检验关于普遍性的假设时，就会建立翻译语料库。然而，对于什么应该或不应该算作翻译以纳入该语料库，却并不清楚。例如，我们需要回答一些棘手的问题来决定这一重大步骤，包括询问如果翻译是由目标语言的母语人士完成的并且是最近出版的，是否应该纳入。问题可能还涉及它们是否可以归类为好或坏，或者它们是否由受过训练的专业人士、业余爱好者、团体、粉丝或个人完成，并包含改编或版本。在构建这样的语料库时，需要明确在何处划清纳入项目的界限。 -->

## 拡張意味単位のから見る現代語訳よる可視化システム

拡張意味単位の観点から、現代語訳は、コロケーションのレベルで対応が一貫しているが、コリゲーションと意味志向のレベルで要素の置き換えと追加など、対応の多様化が進み、談話韻律のレベルでは基調が保持されていると考えられる。現代語訳において、訳者や翻訳観に左右されない追加要素が、コリゲーションと意味志向のレベルにおいて解釈を行った産物として捉えられる。
よって、機能的な表現として、コリゲーション：naked verb
内容的表現として、意味志向のネットワーク、グラフの拡張は、概念レベルで行う。

## 和歌の現代語訳の何が特殊か

古語雅語の同言語系統の翻訳

注釈における翻訳

## 問題点

伊勢物語の「過剰な意味付け」について。78段の「夜のおまし」から。
過剰に意味を付けることは、単なる解釈の違いを超えて、文学研究や文献解釈において問題となることがある。
特に、文献の本来の意図や文脈を歪める危険性がある。
以下に、過剰な意味付けの問題点をいくつか挙げる。

文脈の無視:
まず、文脈に適さない過剰な意味付けは、元の文章の意図や内容を誤解させる可能性がある。
特に、歴史的背景や文化的文脈を無視すると、誤った解釈を誘導する可能性がある。

解釈の妥当性の喪失:
研究者が独自の解釈を過度に推し進めると、その解釈が妥当性を失い、学術的な信頼性が低下し、他の解釈や資料と矛盾する場合、解釈の整合性が疑われることにもなる。

テキストの本質的な意味の曖昧化:
本来のテキストのシンプルな意味や意図が、過剰な意味付けによって曖昧になり、読者がテキストを正しく理解することが難しくなりうる。

研究の信頼性の損失:
誤った解釈や過剰な意味付けに基づいた研究は、学問全体の信頼性を損なうリスクもあり、これにより、他の研究者や読者がその研究を参考にすることに対して懐疑的になる可能性がある。

研究の質の低下:
よって、過剰な意味付けは、単なる解釈の違い以上に、研究の質や信頼性に影響を与える問題である。解釈においては、文脈に忠実であり、テキストの本来の意味を尊重することが重要である。


## 分類体系の有用性

本稿における一致率の計算では、単語アライメントモデルより安定的な分類番号を用いた。分類体系が、同一の意味の語を階層的に対応づけることの有用性が古語研究で再確認できた。現在 @Asahara2022 によって古語への分類語彙番号が付与されていることによって広げられる研究の可能性が十分期待できるであろう。ただし、現在分類語彙表番号は、概念レベルまでであって、類義語、同義語、同語異表記の検出への対応が研究者各自で目標んそった追加的付与が求められている。こん点において、意味体系の作り方に関して検討する余地がある。

# 結論

本稿では、古今和歌集の注釈書における現代語訳の概観と分類を行い、それら現代語訳における追加要素（補足説明）の比例の統計モデリングを行い、事例に対してコーパス言語学的な分析を行った。

# 附録 {.appendix .unnumbered}

## 追加率の予測モデル {.appendix .unnumbered}

$$
\begin{aligned}
&\text{[Likelihood]} \\
\text{Addition Rate}_i &\sim \operatorname{Beta}(\mu_i \phi_i, (1 - \mu_i) \phi_i) \\
\ \\
&\text{[} \mu \text{ part of beta distribution]} \\
\mu_i &= \operatorname{logit}^{-1}(\eta_i) = \frac{1}{1 + e^{-\eta_i}} \\
\eta_i &=  \alpha_{\text{translator}[i]} + \gamma_{\text{poem}[i]} + \beta_{\text{translation focus}[i]} \\
\ \\
&\text{[} \phi \text{ part of beta distribution]} \\
\log(\phi_i) &= \gamma^{\phi}_{\text{poem}[i]} \\
\ \\
& \text{[Group-specific intercepts]} \\
\alpha_{j} &\sim \mathcal{N}(\mu_{\alpha_j}, \sigma_{\text{translator}}), \text{ for } j \text{ in translator} 1, 2, \dots, 10 \\
\gamma_{j} &\sim \mathcal{N}(0, \sigma_{\text{poem}}), \text{ for } j \text{ in poem} 1, 2, \dots, 1000 \\
\gamma^{\phi}_{j} &\sim \mathcal{N}(\mu_{\gamma^{\phi}_{j}}, \sigma^{\phi}_{\text{poem}}), \text{ for } j \text{ in poem} 1, 2, \dots, 1000 \\
\ \\
& \text{[Prior for fixed coefficients]} \\
\beta_{j} &\sim \text{Student-t}(3, 0, 2.5), \text{ for } j \text{ in translation focus} 1, 2, 3, 4\\
\ \\
& \text{[Prior for population-level intercepts]} \\
\mu_{\alpha_{j}}, \mu_{\gamma^{\phi}_{j}} &\sim \text{Student-t}(3, 0, 2.5) \\
\ \\
& \text{[Prior for hyperparameter]} \\
\sigma_{\text{translator}}, \sigma_{\text{poem}}, \sigma^{\phi}_{\text{poem}} &\sim \text{Student-t}(3, 0, 2.5)
\end{aligned}
$$

ここ^[
モデルの書き方として Centered Parameterization にしているが、`brms` での実装が `Non-Centered Parameterization` になっている点に注意されたい。]では：

- $\text{Addition Rate}_i$ は追加率であり、ベータ分布に従うと仮定する。
- $\mu_i \phi_i, (1 - \mu_i) \phi_i$ はそれぞれベータ分布のシェープパラメータに相当する。
- $\mu_i$ はベータ分布の平均値パラメータであり、線形予測子 $\eta_i$ をロジスティック変換することで得られる。
- $\beta_{\text{traslation focus}[i]}, \text{traslation focus}=1,2,3,4$ は4つの翻訳観の固定効果項であり、翻訳観の種類ごとに異なる効果をモデル化する。
- $\alpha_{\text{traslator}[i]}, translator=1,2,\dots,10$ は訳者のランダム効果項（ランダム切片）であり、 
- $\gamma_{\text{poem}[i]}, \text{poem}=1,2,\dots,1000$ は詩（PoemID）のランダム効果項（ランダム切片）である。
- $\phi_i$ はベータ分布の精度パラメータであり、詩（PoemID）のランダム効果項 $\gamma^{\phi}_{\text{poem}[i]}$ でモデル化する。
- $\mu_{\alpha_j}$, $\mu_{\gamma^{\phi}_{j}}$ はそれぞれ $\mu$ パートと $\phi$ パートの population-level の切片である。

事前分布の設定について、基本的に`brms`のデフォルト設定に従う：
- 切片 $\mu_{\alpha_j}$, $\mu_{\gamma^{\phi}_{j}}$ と翻訳観ごとの固定効果 $\beta_{\text{traslation focus}}$ のは Student-t 分布の事前分布に従うと仮定する（`brms` のデフォルト事前分布）。
- $\sigma_{\text{translator}}$, $\sigma_{\text{poem}}$, $\sigma^{\phi}_{\text{poem}}$ はそれぞれ、訳者および詩歌、精度パラメータに対するランダム効果項の標準偏差であり、Student-t 分布に従うと仮定する（`brms` のデフォルト事前分布）。

デフォルト設定が次表を参照する：

```{R}
#| label: tbl-model-configs
#| tbl-scap: デフォルト事前分布
#| warining: false

formula |> get_prior(
  formula, data=data, family = Beta(),
  prior= prior
  ) |> 
  kable()
```

モデルの詳細：

```{R}
#| label: model-info

model |> summary()
```

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- &\text{[Likelihood]} \\ -->
<!-- \text{Addition Rate}_i &\sim \operatorname{Beta}(\mu_i \phi_i, (1 - \mu_i) \phi_i) \\ -->
<!-- \ \\ -->
<!-- &\text{[} \mu \text{ part of beta distribution]} \\ -->
<!-- \mu_i &= \operatorname{logit}^{-1}(\eta_i) = \frac{1}{1 + e^{-\eta_i}} \\ -->
<!-- \eta_i &= + u_{\text{translator}_i} + u_{\text{poem}_i} + \text{\textbf{Translation focus}}_i \boldsymbol{\beta} \\ -->
<!-- \text{\textbf{Translation focus}} &= (\text{Text-focused}\; \text{Translator-focused}\; \text{Reader-focused}\; \text{Others}) \\ -->
<!-- \boldsymbol{\beta} &= (\beta_{0}\; \beta_{1}\; \beta_{2}\; \beta_{3})^{\intercal} \\ -->
<!-- \ \\ -->
<!-- &\text{[} \phi \text{ part of beta distribution]} \\ -->
<!-- \log(\phi_i) &= u_{\phi_{\text{poem}_i}} \\ -->
<!-- \ \\ -->
<!-- & \text{[Translator- and Poem-specific random intercepts]} \\ -->
<!-- u_{\text{translator}_i} &\sim \mathcal{N}(\mu_{\text{translator}}, \sigma_{\text{translator}}^2) \\ -->
<!-- u_{\text{poem}_i} &\sim \mathcal{N}(\mu_{\text{poem}}, \sigma_{\text{poem}}^2) \\ -->
<!-- u_{\phi_{\text{poem}_i}} &\sim \mathcal{N}(\mu_{\phi_{\text{poem}}}, \sigma_{\phi_{\text{poem}}}^2) \\ -->
<!-- \ \\ -->
<!-- &\text{[Priors]} \\ -->
<!-- \beta_{0} &\sim \text{Student-t}(3, 0, 2.5) \\ -->
<!-- \beta_{1}, \beta_{2},\beta_{3} &\sim \operatorname{Uniform}(0, 1) \\ -->
<!-- \sigma_{\text{translator}}, \sigma_{\text{poem}}, \sigma_{\phi_{\text{poem}}} &\sim \text{Student-t}(3, 0, 2.5) -->
<!-- \end{aligned} -->
<!-- $$ -->


<!-- - $\text{Addition Rate}_i$ は追加率であり、ベータ分布に従うと仮定する。 -->
<!-- - $\mu_i \phi_i, (1 - \mu_i) \phi_i$ はそれぞれベータ分布の形状パラメータに相当する。 -->
<!-- - $\mu_i$ はベータ分布の平均値パラメータであり、線形予測子 $\eta_i$ をロジスティック変換することで得られる。線形予測子 $\eta_i$ において、 -->
<!--   - $\boldsymbol{\beta}$ は翻訳観の固定効果項である。 $\beta_{1},\beta_{2},\beta_{3},\beta_{4}$ で構成される。 -->
<!--   - $u_{\text{translator}_i}$ は訳者のランダム効果項（ランダム切片）であり、 $u_{\text{poem}_i}$ はランダム効果項（ランダム切片）である。 -->
<!-- - $\phi_i$ はベータ分布の精度パラメータであり、詩 (PoemID) のランダム効果項 $u_{\phi_{\text{poem}_i}}$ で予測される。 -->
<!-- - 固定効果項 $\boldsymbol{\beta}$ の各要素は Student-t 分布の事前分布を用いる（`brms`のデフォルト事前分布）。 -->
<!-- - $\sigma_{\text{translator}}$, $\sigma_{\text{poem}}$, $\sigma_{\phi_{\text{poem}}}$ はそれぞれ、訳者および詩歌、精度パラメータに対するランダム効果項の標準偏差であり、Student-t 分布に従うと仮定される（`brms`のデフォルト事前分布）。 -->


<!-- $$\text{AdditionRate}_i \sim \text{Beta}(\mu_i \cdot \phi_i, (1 - \mu_i) \cdot \phi_i)$$ -->
<!-- $$\mu_i = \text{logit}^{-1}(\eta_i) = \frac{1}{1 + e^{-\eta_i}}$$ -->
<!-- $$\eta_i = \beta_0 + \beta_{\text{focus}} \cdot \text{focus}_i + u_{\text{translator}_i} + u_{\text{poem}_i}$$ -->
<!-- $$\phi_i = \phi_0 + u_{\phi_{\text{poem}_i}}$$ -->
<!-- $$\beta_0, \beta_{\text{focus}}, \phi_0 \sim \text{Student-t}(3, 0, 2.5)$$ -->
<!-- $$u_{\text{translator}_i} \sim \mathcal{N}(0, \sigma_{\text{translator}}^2)$$ -->
<!-- $$u_{\text{poem}_i} \sim \mathcal{N}(0, \sigma_{\text{poem}}^2)$$ -->
<!-- $$u_{\phi_{\text{poem}_i}} \sim \mathcal{N}(0, \sigma_{\phi_{\text{poem}}}^2)$$ -->
<!-- $$\sigma_{\text{translator}}, \sigma_{\text{poem}}, \sigma_{\phi_{\text{poem}}} \sim \text{Student-t}(3, 0, 2.5)$$ -->

<!-- ここで: -->

<!-- - $\text{Addition Rate}_i$ は追加率であり、ベータ分布に従う。 -->
<!-- - $\mu_i$ はベータ分布の平均値であり、線形予測子 $\eta_i$ をロジスティック変換することで得られる。 -->
<!-- - $\phi_i$ はベータ分布の精度パラメータであり、詩歌 ($\text{PoemID}$) ごとに異なるランダム効果 $u_{\phi_{\text{poem}_i}}$ を持つと仮定される。 -->
<!-- - $\mu_i \cdot \phi_i, (1 - \mu_i) \cdot \phi_i$ はそれぞれベータ分布の形状パラメータに相当する。 -->
<!-- - $\eta_i$ は線形予測子であり、翻訳観の固定効果 ($\beta_{\text{focus}}$)、訳者のランダム効果 ($u_{\text{translator}_i}$)、および詩歌のランダム効果 ($u_{\text{poem}_i}$) を含む。 -->
<!-- - $\phi_i$ は詩歌レベルでのランダム効果を持つ精度パラメータであり，$\phi_0$ という切片と詩歌のランダム効果の和として表現される。 -->
<!-- - $\beta_0$ は切片であり、追加率の平均を示す。 -->
<!-- - $\beta_{\text{focus}}$ は翻訳観の回帰係数であり、翻訳観が追加率に与える固定効果を評価する。 -->
<!-- - $\phi_0$ は精度パラメータ $\phi_i$ の切片であり、Student-t 分布に従うと仮定される。 -->
<!-- - $u_{\text{translator}_i}$ は訳者のランダム効果であり、訳者間の変動を制御する。これは正規分布に従うと仮定される。 -->
<!-- - $u_{\text{poem}_i}$ は詩歌のランダム効果であり、詩歌間の変動を制御する。これは正規分布に従うと仮定される。 -->
<!-- - $u_{\phi_{\text{poem}_i}}$ は精度パラメータ $\phi_i$ の詩歌ごとのランダム効果であり、正規分布に従うと仮定される。 -->
<!-- - $\sigma_{\text{translator}}$, $\sigma_{\text{poem}}$, $\sigma_{\phi_{\text{poem}}}$ はそれぞれ、訳者および詩歌、精度パラメータに対するランダム効果の標準偏差であり、Student-t 分布に従うと仮定される。 -->

